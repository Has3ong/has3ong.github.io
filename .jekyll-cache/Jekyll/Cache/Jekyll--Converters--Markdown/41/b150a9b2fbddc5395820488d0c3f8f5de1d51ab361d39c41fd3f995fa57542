I"<p><em>이 포스트는 <a href="">Optimizing Java</a> 를 바탕으로 작성하였습니다.</em></p>

<p>성능테스트의 좋은 패턴과 안좋은 영향을 끼치는 안티패턴을 열거하고, 안티패턴이 문제가 되지 않도록 리팩터링하는 방법을 설명하겠습니다.</p>

<h2 id="types-of-performance-test">Types of Performance Test</h2>

<p>좋은 성능 테스트는 <strong>정량적(Quantitative)</strong> 입니다.</p>

<p>가장 일반적인 성능 테스트를 알아보겠습니다.</p>

<ul>
  <li>Latency Test</li>
  <li>Throughput Test</li>
  <li>Load Test</li>
  <li>Stress Test</li>
  <li>Endurance Test</li>
  <li>Capacity Planning Test</li>
  <li>Degradation</li>
</ul>

<p>각각의 테스트 유형에 대해 알아보겠습니다.</p>

<h3 id="latency-test">Latency Test</h3>

<p>지연 테스트는 가장 일반적인 성능 테스트입니다.</p>

<p><em>고객이 트랜잭션을 얼마나 오래 참고 기다려야 하는지</em> 측정한 시스템 수치는 가장 피부로 느껴지기 때문입니다.</p>

<h3 id="throughput-test">Throughput Test</h3>

<p>처리율 테스트는 그 다음으로 일반적인 성능 테스트입니다. 다른 측면에서는 처리율은 지연과 동등한 개념이라고 볼 수 있습니다.</p>

<p>이를테면, 지연 테스트를 수행할 때에는 계속 진행 중인 동시 트랜잭션을 반드시 명시해야 합니다.</p>

<blockquote>
  <p>시스템 지연을 측정할때 반드시 처리율을 어느 정도 수준으로 유지했는지 함께 기술해야 합니다.</p>
</blockquote>

<p>마찬가지로, 처리율 역시 지연을 모니터링하면서 테스트합니다. 지연 분포가 갑자기 변하는 시점. 즉, 사실상 한계점이 바로 <strong>최대 처리율</strong> 입니다. 스트레스 테스트의 목표는 이런 현상이 발생하는 지점과 그 시점의 부하 수준을 포착하는 것입니다.</p>

<p>반면, 처리율 테스트는 시스템 성능이 급락하기 직전, 최대 처리율 수치를 측정하는 것이 목표입니다.</p>

<h3 id="load-test">Load Test</h3>

<p>부하 테스트는 처리율 테스트와 조금 다릅니다. <em>시스템이 이 정도 부하를 견딜 수 있을까 없을까?</em> 라는 질문에 답을 구하는 과정입니다. 어플리케이션 트래픽이 상당할 것으로 예상되는 특정 비즈니스 이벤트(ex: 바이럴 컨텐츠)에 대비하기 위해 부하 테스트를 측정합니다.</p>

<h3 id="stress-test">Stress Test</h3>

<p>스트레스 테스트는 시스템 여력이 어느 정도인지 알아보는 수단입니다. 보통 일정한 수준의 트랜잭션, 즉 특정 처리율을 시스템에 계속 걸어놓습니다. 시간이 길수록 서서히 동시 트랜잭션이 증가하고 시스템 성능은 저하됩니다.</p>

<p>측정값이 나빠지기 시작하기 직전의 값이 바로 최대 처리율입니다.</p>

<h3 id="endurance-test">Endurance Test</h3>

<p><strong>Memory Leak</strong>, <strong>Cache Pollution</strong>, <strong>Memory Fragmentation</strong> 등 시간이 지나고 나서야 드러나는 문제점도 있습니다.</p>

<p>대개 이런 종류의 문제는 내구테스트로 감지합니다. 평균 사용률로 시스템에 일정 부하를 계속 주며 모니터링하다가 갑자기 리소스가 고갈되거나 시스템이 깨지는 지점을 찾습니다.</p>

<p>내구 테스트는 빠른 응답을 오규하는 시스템에서 많이 합니다.</p>

<h3 id="capacity-planning-test">Capacity Planning Test</h3>

<p>용량 계획 테스트는 스트레스 테스트와 여러모로 비슷하지만, 분명히 구분되는 차이점이 있습니다. 스트레스 테스트는 현재 시스템이 어느정도 버틸 수 있는지 알아보는 반면, 용량 계획 테스트는 업그레이드한 시스템이 어느 정도 부하를 감당할 수 있을지 미리 내다보는겁니다.</p>

<p>따라서 어떤 이벤트나 위협 요소에 대응하는 것이 아니라, 예정된 계획의 일부분으로 실행하는 경우가 많습니다.</p>

<h3 id="degradation-test">Degradation Test</h3>

<p>저하 테스트는 부분 실패 테스트라고도 합니다. 기본적으로 평상시 운영 환경과 동등한 수준의 부하를 시스템에 가하는 도중, 어떤 컴포넌트나 전체 서브시스템이 갑자기 능력을 상실하는 시점에 벌어지는 일들을 확인합니다.</p>

<p>저하 테스트 도중 눈여겨 봐야할 측정값은 트랜잭션 지연 분포와 처리율입니다.</p>

<p>부분 실패 테스트 중에는 <strong>카오스 멍키(Chaos Monkey)</strong> 라는 하위 유형이 있습니다. 넷플릭스에서 자사 인프라의 견고함을 검증하기 위해 수행한 프로젝트 명에서 유래했습니다.</p>

<p>카오스 멍키의 요지는 진짜 복원성 있는 아키텍처에서는 어느 한 컴포넌트가 잘못돼도 다른 컴포넌트까지 연쇄적으로 무너뜨리면서 전체 시스템에 부정적 영향을 끼치는 일은 없어야 한다는 겁니다.</p>

<p>실제로 운영 환경에 떠 있는 라이브 프러세스를 하나씩 랜덤하게 죽이며 검증합니다.</p>

<p>카오스 멍키형 시스템을 잘 구축하려면 조직 차원에서 시스템 위생, 설계, 운영, 탁월성을 최고 수준으로 확보해야 합니다. 점점 더 많은 기업과 팀들이 관심을 가지는 분야입니다.</p>

<h2 id="best-practices-primer">Best Practices Primer</h2>

<p>성능 튜닝시 주안점을 두어야 할 부분은 다음 3 가지 원칙에 따라 결정합니다.</p>

<ul>
  <li>Identify what you care about and figure out how to measure it.</li>
  <li>Optimize what matters, not what is easy to optimize.</li>
  <li>Play the big points first.</li>
</ul>

<h3 id="top-down-performance">Top-Down Performance</h3>

<p>Java 어플리케이션을 대규모로 벤치마킹하는 일이 작은 코드 섹션별로 정확하게 수치를 얻는것보다 쉽습니다.</p>

<blockquote>
  <p>전체 어플리케이션의 성능 양상부터 먼저 알아보는 방식을 <strong>하양식 성능(Top-Down Performance)</strong> 라 합니다.</p>
</blockquote>

<p>하향식 성능 접근 방식의 성과를 극대화하려면, 먼저 테스트림이 테스트 환경을 구축한 다음, 무엇을 측정하고 무엇을 최적화 해야 하는지, 또 성능 활동을 전체 소프트웨어 개발 주기에서 어떻게 병행해야 하는지, 전 팀원이 명확하게 이해해야 합니다.</p>

<h3 id="creating-a-test-environment">Creating a Test Environment</h3>

<p>테스트 환경 구축은 성능 테스트팀이 가장 먼저 해야할 일입니다. 테스트 환경은 운영 환경과 똑같이 복제해야 합니다.</p>

<p>편의상 기존 QA 환경을 성능 테스트 환경으로 재활용하거나 시간을 나눠쓰려고 하는 경우도 있습니다. 소규모 환경이나 일회성 테스트라면 큰 문제는 안되지만, 오버헤드, 스케줄링, 로지스틱스 문제가 생길 가능성을 절대 과소평과해선 안됩니다.</p>

<p>과거 환경에서는 운영환경과 유사한 성능 테스트 환경을 구축하기 수월했습니다. 운영 환경에서 가동 중인 기계 수만큼 구매해 정확히 운영 환경과 똑같이 설정 해주면 됐습니다.</p>

<p>최근 클라우드 기술의 출현으로 기존 판도가 바뀌었습니다. 주문형 및 자동 확장 인프라 기술이 발전하여 이제는 <em>서버를 구매하고, 네트워크 다이어그램, 소프트웨어를 하드웨어에 배포</em> 공식에 안 맞는 아키텍처가 증가합니다. <strong>데브옵스(DevOps)</strong> 문화가 확산되며, 인프라를 보다 동적으로 관리하는 방식이 각광받기 시작했습니다.</p>

<h3 id="identifying-performance-requirements">Identifying Performance Requirements</h3>

<p>성능을 평가하는 지표는 코드 관점에서만 생각해선 안 되고, 시스템을 전체적으로 바라보며 고객과 경영진에게 중요한 측정값을 고려해야 합니다. 최적화하려는 핵심 지표를 <strong>성능 비기능 요건(NonFunctional Requirement, NFR)</strong> 이라고 합니다.</p>

<p>어떤 목표는 아주 명확합니다.</p>

<ul>
  <li>Reduce 95% percentile transaction time by 100 ms.</li>
  <li>Improve system so that 5x throughput on existing hardware is possible.</li>
  <li>Improve average response time by 30%.</li>
</ul>

<p>하지만, 모호한 목표도 있습니다.</p>

<ul>
  <li>Reduce resource cost to serve the average customer by 50%.</li>
  <li>Ensure system is still within 25% of response targets, even when application clusters are degraded by 50%.</li>
  <li>Reduce customer “drop-off” rate by 25% per 25 ms of latency.</li>
</ul>

<p>모호한 목표를 대상으로 측정대상과 목표를 명확하게 하기 위해 논의하는 자리도 필요합니다.</p>

<h3 id="java-specific-issues">Java-Specific Issues</h3>

<p>성능 분석이라는 과학은 최신 소프트웨어 시스템의 어디에도 적용할 수 있습니다. 하지만, JVM 에는 성능 엔지니어가 주의 깊게 살펴봐야 할 부분들이 있습니다. 메모리 영역의 동적 튜닝 등 JVM 특유의 다이나믹한 자기 관리 기능이 추가되며 복잡해진 까닭입니다.</p>

<p>특히, JIT 컴파일은 유심히 살펴봐야합니다. 최신 JVM 은 어떤 메소드를 JIT 컴파일하여 최적화환 기계어로 변환할지 분석합니다. JIT 컴파일 안하기로 결정된 메소드는 아래 둘 중 하나입니다.</p>

<ul>
  <li>It is not being run frequently enough to warrant being compiled.</li>
  <li>The method is too large or complex to be analyzed for compilation.</li>
</ul>

<p>첫 번째 조건보다 두 번째 조건이 훨씬 드물지만, JVM 기반 어플리케이션에서 성능 활동을 시작하는 첫 단추는, 어떤 메소드가 컴파일 중인지 로그를 남겨 살피고 핵심 코드 경로상 중요 메소드가 잘 컴파일 되고 있는지 확인하는 것입니다.</p>

<h3 id="performance-testing-as-part-of-the-sdlc">Performance Testing as Part of the SDLC</h3>

<p>수준 높은 팀일수록 성능 테스트를 전체 <strong>SDLC(Software Development Life Cycle)</strong> 의 일부로 수행하며 특히 <strong>성능 회귀 테스트(Performance Regression Testing)</strong> 를 상시 수행합니다.</p>

<p>그렇게 하기 위해선 개발팀과 인프라팀이 서로 조율하여 어느 시점에 몇 버전 코드를 성능 테스트 환경에 배포할지 조성해야 합니다. 전용 테스트 환경이 없으면 사실상 불가능한 일입니다.</p>

<h2 id="introducing-performance-antipatterns">Introducing Performance Antipatterns</h2>

<p>안티패턴은 사람들이 수 많은 프로젝트를 하며 밝혀낸 소프트웨어 프로젝트 또는 팀의 좋지 않은 패턴입니다.</p>

<p>성능 튜닝은 항상 초기 기획 단계부터 구체적으로 목표를 정해놓고 시작하는 목표 지향형 프로세스로 접근해야 합니다.</p>

<h3 id="boredom">Boredom</h3>

<p>개발자의 지루함은 프로젝트에 여러가지 해악을 끼칠 수 있습니다. 예를 들어, 지루함을 찾지 못하고 지금까지 알려지지 않은 기술로 컴포넌트를 제작하거나, 필요 이상으로 복잡하게 코딩하는 경우입니다.</p>

<h3 id="résumé-padding">Résumé Padding</h3>

<p>구직 시장에 뛰어든 개발자가 어떻게 하면 자기 연봉과 몸값을 높일 수 있을까 생각하여 이력서 부풀리기를 하는 경우도 있습니다.</p>

<h3 id="peer-pressure">Peer Pressure</h3>

<p>가급적 실수를 안하려는 신입 개발자, 특정 주제를 자기가 잘 모른다는 사실을 두려워하는 개발자가 있습니다. 경쟁심이 불타오르는 팀 분위기 속에서 개발이 광속으로 진행되는 듯 보이고자 제대로 사정을 따져보지 않고 섣불리 중요한 결정을 내리는 것도 Peer Pressure 중 하나의 형태입니다.</p>

<h3 id="lack-of-understanding">Lack of Understanding</h3>

<p>새로운 기술에 대한 지식이 부족한 상태로 <em>다른 프로젝트에서도 이 기술을 사용하더라</em> 와 같은 경험담을 들려주며 이 기술이 적합하다고 쉽게 단정하는것은 아주 위험합니다. 이해가 부족한 상태에서 너무 복잡하게 사용하면 결국 운영 단계에서 회복이 불가능한 중단 사태를 맞이할 수 있습니다.</p>

<h3 id="misunderstoodnonexistent-problem">Misunderstood/Nonexistent Problem</h3>

<p>문제 자체를 제대로 이해하지 못한 채 오로지 기술을 이용해 문제를 해결하려는 개발자도 있습니다. 성능 지표를 수집 / 분석 해야만 문제의 본질을 정확히 이해할 수 있습니다.</p>

<p>안티패턴을 예방하기 위해선 팀원 모두 참여해 기술 이슈를 활발히 공유하는 분위기를 적극 장려해야합니다. 최신 기술이 아무리 좋아 보여도 정확한 정보를 근거로 결정을 내리는게 좋습니다.</p>

<h2 id="performance-antipatterns-catalogue">Performance Antipatterns Catalogue</h2>

<p>갖가지 성능 안티패턴을 유형별로 간략히 소개하겠습니다.</p>

<h3 id="distracted-by-shiny">Distracted by Shiny</h3>

<p><strong>DESCRIPTION</strong></p>

<p>일단 최신의 멋진 기술을 튜닝 타깃으로 정합니다. 개발자는 새로 나온 어플리케이션 컴포넌트를 찾아 헤매는 강박에 사로잡힙니다.</p>

<p><strong>EXAMPLE COMMENTS</strong></p>

<ul>
  <li><em>It’s teething trouble—we need to get to the bottom of it.</em></li>
</ul>

<p><strong>REALITY</strong></p>

<ul>
  <li>어플리케이션을 측정하고 튜닝하려 하지 않고 그냥 어떻게 해보면 되겠지, 하고 생각합니다.</li>
  <li>개발자는 신기술을 제대로 알지 못한 상태에서 문서도 보지 않고 어설프게 지레짐작만 합니다.</li>
  <li>신기술에 관란 온라인 예제는 보통 작은 규모의 전형적인 데이터 셋을 다룹니다. 기업 규모로 확장하는 데 필요한 베스트 프랙티스는 일언반구 말도 없습니다.</li>
</ul>

<p><strong>DISCUSSION</strong></p>

<p>신생팀이나 숙련도가 떨어지는 팀에서 흔합니다. 자신의 능력을 증명하려는 욕구, 레거시 시스템이라고 간주한 것에 메이지 않으려는 마음으로 최신의 기술을 숭배합니다. 보통 새 직장에서 고액의 연봉을 받기 좋은 기술인 경우가 많습니다.</p>

<p><strong>RESOLUTIONS</strong></p>

<ul>
  <li>측정을 하여 진짜 성능 병목점을 찾는다.</li>
  <li>새 컴포넌트 전후로 충분한 로그를 남긴다.</li>
  <li>베스트 프랙티스 및 단순화한 데모를 참조한다.</li>
  <li>팀원들이 새 기술을 이해하도록 독려하고 팀 차원의 베스트 프랙티스 수준을 정한다.</li>
</ul>

<h3 id="distracted-by-simple">Distracted by Simple</h3>

<p><strong>DESCRIPTION</strong></p>

<p>전체적으로 어플리케이션을 프로파일링하여 객관적으로 위험한 부분을 확인하지 않고 시스템에서 제일 간단한 부분만 파고듭니다.</p>

<p><strong>EXAMPLE COMMENTS</strong></p>

<ul>
  <li><em>Let’s get into this by starting with the parts we understand.</em></li>
  <li><em>John wrote that part of the system, and he’s on holiday. Let’s wait until he’s back to look at the performance.</em></li>
</ul>

<p><strong>REALITY</strong></p>

<ul>
  <li>원개발자는 그 시스템 파트를 어떻게 튜닝해야 할지 압니다.</li>
  <li>다양한 시스템 컴포넌트에 대해 지식 공유를 하지 않고 Pair Programming 을 안 한 결과, 독보적인 전문가만 양산됩니다.</li>
</ul>

<p><strong>DISCUSSION</strong></p>

<p>시스템을 정상 가동시키는 일이 주임무인, 체계가 잘 잡힌 유지보수 팀에서 흔히 나타나는 안티패턴입니다. 어플리케이션에 신기술을 적용하고 새로 병합할 경우, 개발자들은 새 시스템을 안 맡으려고 합니다.</p>

<p>이런 분위기에서 개발자는 시스템에서 자기가 익숙한 부분만 프로파일링 하는 게 더 속편합니다. 포근한 울타리를 벗어나지 않고도 원하는 결과를 운 좋게 얻게 되기를 간절히 바랍니다.</p>

<p><strong>RESOLUTIONS</strong></p>

<ul>
  <li>측정을 하여 진짜 성능 병목점을 찾는다.</li>
  <li>본인이 익숙지 않은 컴포넌트에 문재가 생길 시 전문가에게 도움을 청한다.</li>
  <li>개발자가 전체 시스템 컴포넌트를 고루 이해하도록 독려한다.</li>
</ul>

<h3 id="performance-tuning-wizard">Performance Tuning Wizard</h3>

<p><strong>DESCRIPTION</strong></p>

<p>전문가를 수소문해 채용합니다. 그러곤 그 명성에 걸맞는 탁월한 튜닝 스킬로 사내 모든 성능 이슈를 바로잡고 회사를 정상 궤도에 올려놓으라는 특명을 내립니다.</p>

<p><strong>EXAMPLE COMMENTS</strong></p>

<ul>
  <li><em>I’m sure I know just where the problem is…</em></li>
</ul>

<p><strong>REALITY</strong></p>

<ul>
  <li>명망 높은 높은 마법사나 초인이 하는 일이라곤 드레스코드에 도전하는 일이 고작입니다.</li>
</ul>

<p><strong>DISCUSSION</strong></p>

<p>이 안티패턴은 성능 이슈를 해결하기에 실력이 조금 부족하다고 여기는 개발 팀원들이 소외감을 느끼게 합니다. 사실, 프로파일링을 하고 조금만 최적화해도 성능 향상의 혜택을 볼 수 있는데 말입니다.</p>

<p>특정 이슈를 해결한 방법이나 자기가 알고 있는 걸 절대로 남과 공유 안 하려는, 초인을 지향하는 팀원은 매우 반생산적입니다.</p>

<p><strong>RESOLUTIONS</strong></p>

<ul>
  <li>측정을 하여 진짜 성능 병목점을 찾는다.</li>
  <li>새로 채용된 팀내 전문가가 다른 팀원들과 지식을 공유하고 팀워크를 유지할 수 있게 리드한다.</li>
</ul>

<h3 id="tuning-by-folklore">Tuning by Folklore</h3>

<p><strong>DESCRIPTION</strong></p>

<p>서비스 중 발생한 성능 문제를 해결하려고 노력하던 중 웹사이트에서 발견한 마법의 설정 매개변수를 발견하여 테스트도 안해보고 운영 서버에 적용한다.</p>

<p><strong>EXAMPLE COMMENTS</strong></p>

<ul>
  <li><em>I found these great tips on Stack Overflow. This changes everything.</em></li>
</ul>

<p><strong>REALITY</strong></p>

<ul>
  <li>개발자는 성능 팁의 전후 맥락이나 기초를 모르고 그 팁이 진짜 어떤 영향을 미칠지 모른다.</li>
  <li>어떤 시스템에서 통했을지 모르지만 다른 시스템 적용해도 효과가 있을지 모릅니다.</li>
</ul>

<p><strong>DISCUSSION</strong></p>

<p>한두 글자 바뀌는 설정값이라도 조심해서 관리하지 않으면 운영계에서 큰 사고가 터질 수 있습니다.</p>

<p><strong>RESOLUTIONS</strong></p>

<ul>
  <li>시스템에 가장 중요한 부분에 직접 영향을 미치는 기술은 확실히 파악하고 충분히 검증된 것들만 적용한다.</li>
  <li>매개변수를 UAT 에서 시험한다. 어떤 변화라도 철저히 검증하고 효과를 프로파일링 애햐한다.</li>
  <li>다른 개발자나 운영 요원, 데브옵스팀과 함께 설정 문제를 회의한다.</li>
</ul>

<h3 id="the-blame-donkey">The Blame Donkey</h3>

<p><strong>DESCRIPTION</strong></p>

<p>정작 이슈와 상관없는 특정 컴포넌트를 문제 삼는다.</p>

<p><strong>EXAMPLE COMMENTS</strong></p>

<ul>
  <li><em>It’s always JMS/Hibernate/A_N_OTHER_LIB.</em></li>
</ul>

<p><strong>REALITY</strong></p>

<ul>
  <li>충분히 분석도 안 해보고 성급한 결론을 내린다.</li>
  <li>평소 의심했던 범인을 수사 과정의 유일한 용의자로 지목한다.</li>
  <li>진짜 원인을 밝히려면 숲을 봐야 하는데 그럴 마음이 없다.</li>
</ul>

<p><strong>DISCUSSION</strong></p>

<p>비난의 대상으로 낙인찍힌 것들 이외의 코드 베이스나 라이브러리를 잘 모르는 엔지니어는 이 안티패턴의 제물이 되기 쉽습니다. 아무래도 뭔가 새로 조사하는 것보다 보통 문제를 많이 일으키는 곳을 지목하는 게 속 편합니다. 이런 현상은 팀 전체가 매너리즘에 빠졌다는 신호라 머지않아 갖가지 운영 이슈가 불거집니다.</p>

<p>정확한 원인을 밝히려면 프로파일링이 필수입니다.</p>

<p><strong>RESOLUTIONS</strong></p>

<ul>
  <li>성급한 결론을 내릴려하지 않는다.</li>
  <li>정상적으로 분석한다</li>
  <li>분석 결과를 모든 이해관계자와 논의한다.</li>
</ul>

<h3 id="missing-the-bigger-picture">Missing the Bigger Picture</h3>

<p><strong>DESCRIPTION</strong></p>

<p>전체적인 변경 영향도를 완전히 파악하지 않은 채 일단 변경을 해보거나 어플리케이션의 극소 부분만 프로파일링한다.</p>

<p><strong>EXAMPLE COMMENTS</strong></p>

<ul>
  <li><em>If I just change these settings, we’ll get better performance.</em></li>
  <li><em>If we can just speed up method dispatch time…</em></li>
</ul>

<p><strong>REALITY</strong></p>

<ul>
  <li>변경 영향도를 완전히 이해한 사람이 없다.</li>
  <li>새로 바꾼 JVM 설정값 하에서 어플리케이션을 완전히 프로파일링 하지 않았다.</li>
  <li>마이크로벤치마킹 때문에 빚어질 전체 시스템 영향도를 파악하지 않는다.</li>
</ul>

<p><strong>DISCUSSION</strong></p>

<p>2 가지 사실을 간과해선 안 됩니다. 첫째, 운영계를 그대로 모사한 UAT 환경 없이 최적화의 효용성을 판단하기 어렵습니다. 둘째, 부하가 높을 때만 도움이 되고 평소에는 외려 성능을 떨어트리는 최적화는 아무 의미가 없습니다. 평상시 서비스 이용 실태를 나타내면서도 부하 집중 시 유의미한 테스트를 제공하는 데이터를 얻기란 결코 쉽지 않습니다.</p>

<p><strong>RESOLUTIONS</strong></p>

<p>운영계에서 스위치를 변경하기 전 다음 절차를 따른다.</p>

<ol>
  <li>운영계 성능 지표 측정</li>
  <li>UAT 에서 한번에 스위치 하나씩 변경</li>
  <li>스트레스를 받는 지점이 UAT 와 운영계와 동일한지 확인</li>
  <li>운영계에서 일반적인 부하를 나타내는 테스트 데이터를 확보</li>
  <li>UAT 에서 스위치를 변경하며 테스트</li>
  <li>UAT 에서 재테스트</li>
  <li>추론한 내용을 다른 사람에게 재검토 요청</li>
  <li>결론을 다른 사람과 공유</li>
</ol>

<h3 id="uat-is-my-desktop">UAT Is My Desktop</h3>

<p><strong>DESCRIPTION</strong></p>

<p>UAT 환경이 운영계 환경과 전혀 다른 경우도 많습니다. 개발자는 그 차이점을 예상하거나 완전히 이해하지 못한 채 저성능 데스크톱에서 고성능 운영 서버로 서비스할 코드를 작성한다. 그 결과 개발자 장비보다 코어 수가 훨씬 많고 대용량 RAM 에 비싼 I/O 장비가 탑재되는 고성능 운영계 장비에서 문제가 발생합니다.</p>

<p><strong>EXAMPLE COMMENTS</strong></p>

<ul>
  <li><em>A full-size UAT environment would be too expensive.</em></li>
</ul>

<p><strong>REALITY</strong></p>

<ul>
  <li>UAT 환경이 운영계와달라 서비스가 중단되는 사태가 벌어지면 장비를 추가하는 비용보다 더 값비싼 대가를 치르게 된다.</li>
</ul>

<p><strong>DISCUSSION</strong></p>

<p>유의미한 추정을 하려면 UAT 환경을 운영계와 반드시 동기화 해야합니다.</p>

<p>근래의 환경은 적응성이 좋아 런타임 서브시스템은 리소스를 최대한 활용하지만, 실제로 배포될 환경의 리소스가 UAT 와 많이 차이 나면 그에 맞는 결정을 내릴 거싱ㅂ니다. 결국 장밋빛 희망에 가득 찬 어설픈 추정은 무용지물이 됩니다.</p>

<p><strong>RESOLUTIONS</strong></p>

<ul>
  <li>서비스 중단 비용과 고객 이탈로 인한 기회비용을 비교한다.</li>
  <li>운영 환경과 동일한 UAT 환경을 구입</li>
  <li>소 잃고 외양간 고치는 비용이 더 많을 테니, 관리자에게 올바른 사례를 제시</li>
</ul>

<h3 id="production-like-data-is-hard">Production-Like Data Is Hard</h3>

<p><strong>DESCRIPTION</strong></p>

<p><strong>데이터라이트(DataLite)</strong> 라 하는 이 안티패턴은, 개발자들이 운영계와 유사한 데이터를 나타내고자 할 때 빠지는 함정입니다. UAT 전략을 다음과 같이 수립했을 때 무슨 문제가 벌어질 지 하나씩 살펴보겠습니다. 대형 은행의 무역 거래소에서 예약은 완료됐으나 미지급한 선물 및 옵션 거래를 처리한다고 가정하겠습니다.</p>

<ul>
  <li>테스트 편의상, 하루에 오가는 메세지 중 작은 한 부분만 포착하여 UAT 에 전부 돌려본다.
    <ul>
      <li>시스템에 나타나는 버스트 양상과 다른 시장에서 거래 옵션이 열리기 전 특정 시장에서 더 많은 선물 거래가 일어나서 발생하는 웜업 현상도 포착할 수 없습니다.</li>
    </ul>
  </li>
  <li>테스트 편의상 단언시 단순한 값만 사용하도록 거래, 옵션 데이터를 업데이트한다.
    <ul>
      <li>데이터 현실성이 떨어집니다.</li>
    </ul>
  </li>
  <li>작업 편의상 모든 값을 한꺼번에 시스템에 밀어 넣는다.
    <ul>
      <li>데이터 적재 비율이 나타나는 핵심적인 웜업과 최적화를 놓치게 됩니다.</li>
    </ul>
  </li>
</ul>

<p>UAT 에서 데이터셋을 단순화하여 테스트하는 경우, 쓸 만한 결과를 얻을 확률은 떨어집니다.</p>

<p><strong>EXAMPLE COMMENTS</strong></p>

<ul>
  <li><em>It’s too hard to keep production and UAT in sync.</em></li>
  <li><em>It’s too hard to manipulate data to match what the system expects.</em></li>
  <li><em>Production data is protected by security considerations. Developers should not have access to it.</em></li>
</ul>

<p><strong>REALITY</strong></p>

<p>UAT 에서 정확한 결과를 얻기 위해선 운영계 데이터와 최대한 맞추어야 합니다. 보안 정책 때문에 원본 데이터를 가져올 수 없다면 데이터를 뒤죽박죽 섞어서 테스트하는 방법도 있습니다.</p>

<p><strong>DISCUSSION</strong></p>

<p>시스템을 오픈하고 UAT 데이터로 예상했던 것과는 전혀 다른 패턴을 보이면 운영환경에서 큰 문제가 발생할 수 있습니다.</p>

<p><strong>RESOLUTIONS</strong></p>

<ul>
  <li>데이터 도메인 전문가에게 컨설팅을 받고 운영 데이터를 UAT 로 다시 이전하는 프로세스에 시간과 노력을 투자한다.</li>
  <li>다수의 고객이 몰리고 엄청난 트랜잭션이 예상되는 서비스는 출시 전 철저히 준비한다.</li>
</ul>

<h2 id="cognitive-biases-and-performance-testing">Cognitive Biases and Performance Testing</h2>

<p>인간은 원래 정확하고 신속하게 의견을 정리하는 일에 서투릅니다.</p>

<blockquote>
  <p><strong>인지 편향(Cognitive Bias)</strong> 은 인간의 두뇌가 부정확한 결론을 내리게 이끄는 심리 작용입니다.</p>
</blockquote>

<p>최근 특정 컴포넌트 때문에 시스템이 다운된 사례가 있을 경우, 개발자들은 새로 발생한 성능 문제도 그 컴포넌트가 원인이라는 편견을 가지게 됩니다. 한번 그런 생각을 가지면 분석한 데이터를 보는 자신의 믿음을 점점 더 굳히게 됩니다. 이 안티패턴은 <strong>확중 편향(Confirmation Bias)</strong> 과 <strong>최신 편향(Recency Bias)</strong> 이 조합된 것입니다.</p>

<p>각 편향은 이중적 또는 상보적입니다. 예를들어, <em>UAT 에서는 잘 돌아갔어 그러니 운영계에 문제가있어</em> 라는 사고방식입니다.</p>

<h3 id="reductionist-thinking">Reductionist Thinking</h3>

<p><strong>환원주의(Redductionist Thininkng)</strong> 는 시스템을 아주 작은 조각으로 나누어 그 구성 파트를 이해하면 전체 시스템도 다 이해할 수 있다고 생각하는 분석주의적 사고방식입니다. 각 파트를 이해하면 그릇된 가정을 내릴 가능성도 작을거라는 편향된 논리입니다.</p>

<p>하지만 복잡한 시스템을 그렇지 않다는게 문제입니다. 단순히 구성 파트를 합한 것보다 시스템을 전체로 바라봐야 문제의 원인을 찾을 수 있습니다.</p>

<h3 id="confirmation-bias">Confirmation Bias</h3>

<p><strong>확증편향</strong> 은 성능 면에서 중대한 문제를 초래하거나, 어플리케이션을 주관겆으로 바라보게 합니다. 테스트 세트를 부실하게 선택하거나 테스트 결과를 통계적으로 분석하지 않으면 확증 편향의 포로가 되기 쉽습니다.</p>

<h3 id="fog-of-war-action-bias">Fog of War (Action Bias)</h3>

<p>시스템이 예상대로 작동하지 않거나 중단된 시간 중에 발현되는 편향입니다. 가장 흔한 원인을 뽑아보겠습니다.</p>

<ul>
  <li>Changes to infrastructure that the system runs on, perhaps without notification or realizing there would be an impact</li>
  <li>Changes to libraries that the system is dependent on</li>
  <li>A strange bug or race condition the manifests itself on the busiest day of the year</li>
</ul>

<p>평소 로깅, 모니터링을 꾸준히하며 잘 가꾸어 온 어플리케이션은 오류가 발생해도 명확한 에러 메세지가 생성되므로 지원팀이 금세 원인을 찾을 수 있습니다.</p>

<p>하지만, 실패 시나리오를 충분히 테스트 안 해본 상태로 오픈해 로그도 제대로 남기지 않는 어플리케이션이 많습니다. 이런 상황이면 숙련된 엔지니어도 시스템 중단을 해결하려고 뭐라도 해야 한다는 막연한 느낌으로 서두르기만 하다 절망에 빠질 수 있습니다.</p>

<h3 id="risk-bias">Risk Bias</h3>

<p>인간의 본성은 위험을 피하고 변화를 거부합니다. 누구나 예전에 바꿨더니 잘못됐던 경험을 가지고 있어서 가급적 위험을 감수하려 하지 않습니다. 단위 테스트 세트와 운영계 회귀 테스트 체계만 확실히 갖추어도 리스크를 상당히 줄일 수 있습니다.</p>

<p>위험 편향은 보통 어플리케이션 문제가 생겼을 때 제대로 학습하고 적절한 조치를 하지 못한 까닭에 더 고착화됩니다.</p>

<h3 id="ellsbergs-paradox">Ellsberg’s Paradox</h3>

<p>엘스바그 역설은 인간이 확률을 이해하는 데 얼마나 서트룬지 잘 보여주는 사례입니다.</p>

<p>엘스바그 역설은 실험을 통한 단순확률로 설명할 수 있습니다. 통 안에 색깔있는 90 개 공이 있고, 그중 30 개는 파란 공, 나머지는 빨간 공 아니면 녹색 공이라고 하겠습니다. 빨간 공과 녹색 공 비율은 알 수 없지만, 통은 하나밖에 없고, 전체 공의 개수는 정해져 있으니 확률은 일정합니다.</p>

<p>이 역설의 첫 단계는 어떤 내기를 할지 선택하는 겁니다. 참가자는 다음 내기 중 하나를 선택합니다.</p>

<ol>
  <li>통에서 꺼낸 파란 공이면 상금 $100 를 받는다.</li>
  <li>통에서 꺼낸 빨간 공이면 상금 $100 를 받는다.</li>
</ol>

<p>대부분 사람들은 A 를 선택합니다. 이길 확률이 정확히 1/3 이기 떄문입니다. 그러나 (꺼낸 공을 다시 통에 넣고 섞으면) 참가자가 다음과 같이 두 번째 내기를 할 경우 놀라운 일이 벌어집니다.</p>

<ol>
  <li>통에서 꺼낸 공이 파란 공 또는 녹색 공이면 상금 $100 를 받는다.</li>
  <li>통에서 꺼낸 공이 빨간 공 또는 녹색 공이면 상금 $100 를 받는다.</li>
</ol>

<p>이때 사람들은 대부분 이길 확률이 2/3 이 뻔한 D 를 고릅니다.</p>

<p>하지만 역설적으로 A 와 D 는 비이성적인 선택합니다. A 를 선택한건 빨간 공과 녹색 공이 어떤 분포일 거라는 의사(즉, 녹색 공 &gt; 빨간 공)를 암묵적으로 나타낸 겁니다. 따라서 기왕 A 를 선택했다면 논리적으로 D 보다 이길 확률이 더 높은 C 를 잇따라 선택하는 게 더 우세한 전략입니다.</p>
:ET