I"!<p>기존에는 가중치 매개변수의 기울기는 수치 미분을 사용했습니다. 하지만, 수치 미분은 단순하고 구현하기 쉽지만 계산 시간이 오래걸린다는 단점이 있습니다. <strong>오차역전파법(Backpropagation)</strong> 을 이용하면 가중치 매개변수의 기울기를 효율적으로 계산할 수 있습니다.</p>

<h2 id="computational-graph">Computational Graph</h2>

<p><strong>계산 그래프(Computational Graph)</strong> 는 계산 과정을 그래프로 나타낸 것입니다. 여기서 그래프는 복수의 <strong>노드(Node)</strong> 와 <strong>엣지(Edge)</strong> 로 표현됩니다.</p>

<h3 id="work-out-a-computation-graph">Work Out a Computation Graph</h3>

<blockquote>
  <p>문제 1 : 슈퍼에서 1 개에 100 원인 사과 2 개를 구매했습니다. 이때 지불 금액을 구하세요. 단, 소비세가 10% 부과됩니다.</p>
</blockquote>

<p>계산 그래프는 과정을 노드와 화살표로 표현합니다. 노드는 원(ㅇ) 으로 표기하고 안에 연산 내용을 적습니다. 계산 결과를 화살표 위에 적어 각 노드의 계산 결과가 왼쪽에서 오른쪽으로 전해지게 합니다. 문제 1 을 계산 그래프로 풀면 <code class="highlighter-rouge">Example 1</code> 이 됩니다.</p>

<blockquote>
  <p>Example 1</p>
</blockquote>

<p>위 그림을 <code class="highlighter-rouge">Example 2</code> 처럼 원 밖에 표기할 수 있습니다.</p>

<blockquote>
  <p>Example 2</p>
</blockquote>

<p>다음 문제를 보겠습니다.</p>

<blockquote>
  <p>문제 2 : 슈퍼에서 사과를 2 개, 귤을 3 개 샀습니다. 사과는 1 개에 100 원, 귤은 1 개 150 원입니다. 소비세가 10% 일 때 지불 금액을 구하세요.</p>
</blockquote>

<p>정답을 알아보겠습니다.</p>

<blockquote>
  <p>Example 3</p>
</blockquote>

<p>계산 그래프를 이용한 문제풀이는 다음 흐름으로 진행합니다.</p>

<ol>
  <li>계산 그래프를 구성한다.</li>
  <li>그래프에서 계산을 왼쪽에서 오른쪽으로 진행한다.</li>
</ol>

<p>여기서 2 번째 <em>계산을 왼쪽에서 오른족으로 진행</em> 하는 단계를 <strong>순전파(Foreward Propagation)</strong> 이라고 합니다. 그리고 순전파의 반대를 <strong>역전파(Backward Propogation)</strong> 라고 합니다. 역전파는 이후 미분을 계산할 때 중요한 역할을 합니다.</p>

<h3 id="local-computation">Local Computation</h3>

<p>계산 그래프의 특징은 <strong>국소적 계산</strong> 을 전파함으로 최종 결과를 얻는다는 점에 있습니다.</p>

<blockquote>
  <p>국소적 계산 : 현재 계산이 이전 계산이나 다음 계산의 영향을 받지 않는다.</p>
</blockquote>

<p>국소적 계산을 예를 들어보겠습니다. 슈퍼에서 사과 2 개를 포함한 여러 식품을 구입하는 경우를 알아보겠습니다. <code class="highlighter-rouge">Example 4</code> 처럼 계산 그래프로 나타낼 수 있습니다.</p>

<blockquote>
  <p>Example 4</p>
</blockquote>

<p>각 노드는 자신과 관련된 계산 외에는 아무것도 신경 쓸 게 없습니다. 이처럼 계산 그래프는 국소적 계산에 집중합니다.</p>

<h3 id="advantages-of-computation-graphs">Advantages of Computation Graphs</h3>

<p>계산그래프의 가장 큰 이점은 전체가 아무리 복잡해도 각 노드에서는 단순한 계산에 집중하여 문제를 단순화 시킬 수 있습니다. 실제 계산 그래프를 사용하는 가장 큰 이유는 역전파를 통해 미분을 효율적으로 계산할 수 있는 점에 있습니다.</p>

<p>계산 그래프의 역전파를 설명하기 위해 <code class="highlighter-rouge">문제 1</code> 을 다시 살펴보겠습니다. <code class="highlighter-rouge">문제 1</code> 은 사과를 2개 사서 소비세를 포함한 최종 금액을 구하는것이였습니다.</p>

<p>여기서 가령 사과 가격이 오르면 최종 금액에 어떤 영향을 끼치는지 알고싶다고 합시다. 이는 <em>사과 가격에 대한 지불 금액의 미분</em> 을 구하는 문제입니다.</p>

<p>기호로 나타내면 사과 값을 $x$ 지불 금액을 $L$ 이라 했을 때 ${ \partial L \over \partial x }$ 을 구하는 것입니다. 이 미분 값은 사과 값이 올랐을 때 지불 금액이 얼마나 증가하느냐를 표시한 것입니다.</p>

<p><code class="highlighter-rouge">Example 5</code> 처럼 계싼 그래프 상의 역전파에 의해서 미분을 구할 수 있습니다.</p>

<blockquote>
  <p>Example 5</p>
</blockquote>

<p>역전파는 오른쪽에서 왼쪽으로 <em>1 -&gt; 1.1 -&gt; 2.2</em> 순으로 미분 값을 전달합니다. 이 결과로부터 <em>사과 가겨에 대한 지불 금액의 미분 값</em> 은 <strong>2.2</strong> 라 할 수 있습니다. 사과가 1원 오르면 최종 금액은 2.2 원 오른다는 뜻입니다.</p>

<h2 id="chain-rule">Chain Rule</h2>

<p>위에서 살펴본 국소적 미분을 전달하는 원리가 <strong>연쇄법칙(Chain Rule)</strong> 에 따른 것입니다.</p>

<h3 id="backward-propagation-of-computation-graph">Backward Propagation of Computation Graph</h3>

<p>계산그래프를 사용한 역전파의 예를 하나 살펴보겠습니다. $y = f(x)$ 라는 계산의 역전파는 <code class="highlighter-rouge">Example 6</code> 과 같습니다.</p>

<blockquote>
  <p>Example 6</p>
</blockquote>

<p><code class="highlighter-rouge">Example 6</code>  과 같이 역전파의 계산 절차는 신호 $E$ 에 노드의 국소적 미분 $({ \partial y \over \partial x})$ 을 곱훈하 다음 노드로 전달하는 것입니다. 여기서 말하는 국소적 미분은 순전파 때의 $y = f(x)$ 계산의 미분을 구하는 것이며, 이는 $x$ 에 대한 $y$ 의 미분 $({ \partial y \over \partial x})$ 을 구한다는 의미 입니다.</p>

<p>가령 $y = f(x) = x^2$ 이라면 ${ \partial y \over \partial x} = 2x$ 가 됩니다. 이 국소적인 미분을 상류에서 전달된 값에 곱해 앞쪽 노드로 전달하는 것입니다.</p>

<p>이것이 역전파의 계산 순서인데, 이러한 방식을 따르면 목표로 하는 미분 값을 효율적으로 구할 수 있다는 것이 이 전파의 핵심입니다. 왜 이런 일이 가능한지 연쇄법칙의 원리로 설명하겠습니다.</p>

<h3 id="what-is-a-chain-rule-">What is a Chain Rule ?</h3>

<p>연쇄법칙을 설명하기 위해선 합성 함수부터 시작해야 합니다. <strong>합성 함수</strong> 란 여러 함수로 구성된 함수입니다. 예를 들어 $z = (x + y)^2$ 이라는 식은 <code class="highlighter-rouge">Expression 1</code> 처럼 두 개의 식으로 구성됩니다.</p>

<blockquote>
  <p>Expression 1</p>
</blockquote>

<script type="math/tex; mode=display">z = t^2 \\
t = x + y</script>

<p>연쇄법칙은 합성 함수의 미분에 대한 성질이며, 다음과 같이 정의됩니다.</p>

<blockquote>
  <p>합성 함수의 미분은 합성 함수를 구성하는 각 함수의 미분의 곱으로 나타낼 수 있습니다.</p>
</blockquote>

<p>이것이 연쇄법칙의 원리입니다. <code class="highlighter-rouge">Expression 1</code> 을 예로 설명하면 ${ \partial z \over \partial x}$ ($x$ 에 대한 $z$의 미분) 은 ${ \partial z \over \partial t}$ ($t$ 에 대한 $z$ 의 미분) 과 ${ \partial t \over \partial x}$ ($x$ 에 대한 $t$ 의 미분) 의 곱으로 나타낼 수 있습니다. 수식으로는 <code class="highlighter-rouge">Expression 2</code> 처럼 쓸 수 있습니다.</p>

<blockquote>
  <p>Expression 2</p>
</blockquote>

<script type="math/tex; mode=display">{ \partial z \over \partial x} = { \partial z \over \partial t} { \partial t \over \partial x}</script>

<p>이 식은 아래와 같이 $\partial t$ 를 지울 수 있습니다.</p>

<script type="math/tex; mode=display">{ \partial z \over \partial x} = { \partial z \over \partial x}</script>

<p>연쇄 법칙을 써서 <code class="highlighter-rouge">Expression 2</code> 의 미분 ${ \partial z \over \partial x}$ 를 구해보겠습니다. 먼저 <code class="highlighter-rouge">Expression 1</code> 의 편미분을 구합니다.</p>

<blockquote>
  <p>Expression 3</p>
</blockquote>

<script type="math/tex; mode=display">{ \partial z \over \partial t } = 2t \\
{ \partial t \over \partial x } = 1</script>

<p><code class="highlighter-rouge">Expression 3</code> 과 같이 ${ \partial t \over \partial x}$ 는 1 입니다. 이는 미분 공식에서 해석적으로 구한 결과입니다. 그리고 최종적으로 구하고 싶은 ${ \partial z \over \partial x }$ 는 <code class="highlighter-rouge">Expression 3</code> 에서 구한 두 미분을 곱해 계산합니다.</p>

<blockquote>
  <p>Expression 4</p>
</blockquote>

<script type="math/tex; mode=display">{ \partial z \over \partial x } = { \partial z \over \partial t}
{ \partial t \over \partial x } = 2 t \cdot 1 = 2(x+y)</script>

<h3 id="chain-rule-and-computation-graphs">Chain Rule and Computation Graphs</h3>

<p><code class="highlighter-rouge">Expression 4</code> 의 연쇄법칙 계산을 계산 그래프로 나타내보겠습니다. 2 제곱 계산을 **2 노드로 나타내면 <code class="highlighter-rouge">Example 7</code> 처럼 그릴 수 있습니다.</p>

<blockquote>
  <p>Example 7</p>
</blockquote>

<p><code class="highlighter-rouge">Example 7</code> 과 같이 계산 그래프의 역전파는 오른쪽에서 왼쪽으로 신호를 전파합니다. 역전파의 계산 절차에서는 노드로 들어온 입력 신호에 그 노드의 국소적 미분을 곱한 후 다음 노드로 전달합니다.</p>

<p>예를 들어 **2 노드에서의 역전파를 보겠습니다. 입력은 ${ \partial z \over \partial z}$ 이며, 이에 국소적 미분인 ${ \partial z \over \partial t}$ 를 곱하고 다음 노드로 넘깁니다. 한 가지 <code class="highlighter-rouge">Example 7</code> 에서 역전파의 첫 신호인 ${ \partial z \over \partial z }$ 의 값은 결국 1 이라서 앞의 수식에서 언급하지 않았습니다.</p>

<p><code class="highlighter-rouge">Example 7</code> 에서 주목할 것은 왼쪽 역전파입니다. 이 계산은 연쇄 법칙에 따르면 ${ \partial z \over \partial z } { \partial z \over \partial t} { \partial t \over \partial x} = { \partial z \over \partial t} { \partial t \over \partial x} = { \partial z \over \partial x}$ 가 성립되어 <em>$x$ 에 대한 $z$ 의 미분</em> 이 됩니다.</p>

<p><code class="highlighter-rouge">Example 7</code> 에 <code class="highlighter-rouge">Expression 3</code> 의 결과를 대입하면 <code class="highlighter-rouge">Example 8</code> 이 되며, ${ \partial z \over \partial x }$ 는 $2(x+y)$ 임을 구할 수 있습니다.</p>

<blockquote>
  <p>Example 8</p>
</blockquote>

<h2 id="backward-propagation">Backward Propagation</h2>

<p>$+$ 와 X 등의 연산을 예로 들어 역전파의 구조를 설명하겠습니다.</p>

<h3 id="backward-propagation-of-addition-node">Backward Propagation of Addition Node</h3>

<p>$z = x + y$ 라는 식을 대상으로 역전파를 살펴보겠습니다. 우선 $z = x + y$ 의 미분은 다음과 같이 해석적으로 계산할 수 있습니다.</p>

<blockquote>
  <p>Expression 5</p>
</blockquote>

<script type="math/tex; mode=display">{ \partial z \over \partial x } = 1 \\
{ \partial z \over \partial y } = 1</script>

<p><code class="highlighter-rouge">Expression 5</code> 와 같이 모두 1 이 됩니다. 이를 계산 그래프로 나타내면 <code class="highlighter-rouge">Exapmle 9</code> 처럼 됩니다.</p>

<blockquote>
  <p>Example 9</p>
</blockquote>

<p><code class="highlighter-rouge">Example 9</code> 와 같이 역전파 때는 상류에서 전해진 미분에 1을 곱하여 하류로 흘립니다. 즉, 덧셈 노드의 역전파는 1 을 곱하기만 할 뿐 입력된 값을 그대로 다음 노드로 보냅니다.</p>

<p>상류에서 전해진 미분 값을 ${ \partial L \over \partial z }$ 라 했는데, 이는 <code class="highlighter-rouge">Example 10</code> 과 같이 최종적으로 $L$ 이라는 값을 출력하는 큰 계산 그래프를 가정하기 때문입니다. $z = x+y$ 계산은 그 큰 계산 그래프의 중간 어딘가에 존재하고, 상류로부터 ${ \partial L \over \partial z }$ 값이 전해진 것입니다. 그리고 다시 하류로 ${ \partial L \over \partial x }$ 와 ${ \partial L \over \partial y }$ 값을 전달합니다.</p>

<blockquote>
  <p>Example 10</p>
</blockquote>

<p>가령 <em>10 + 5 = 15</em> 라는 계산이 있고 상류에서 1.3 이라는 값이 흘러옵니다. 이를 계산 그래프로 나타내면 <code class="highlighter-rouge">Example 11</code> 처럼 됩니다.</p>

<blockquote>
  <p>Example 11</p>
</blockquote>

<p>덧셈 노드의 역전파는 입력신호를 다른 노드로 출력할 뿐만 아니라 <code class="highlighter-rouge">Example 11</code> 처럼 1.3 을 그대로 다음 노드에 전달합니다.</p>

<h3 id="backward-propagation-of-multiple-node">Backward Propagation of Multiple Node</h3>

<p>곱셈 노드의 역전파를 설명하겠습니다. $z=xy$ 식을 생각해보겠습니다. 이 식의 미분은 다음과 같습니다.</p>

<blockquote>
  <p>Expression 6</p>
</blockquote>

<script type="math/tex; mode=display">{ \partial z \over \partial x } = y \\
{ \partial z \over \partial y } = x</script>

<p><code class="highlighter-rouge">Expression 6</code> 계산 그래프는 다음과 같이 그릴 수 있습니다.</p>

<blockquote>
  <p>Example 12</p>
</blockquote>

<p>곱셈 노드 역전파는 상류의 값의 순전파 때의 입력 신호들을 서로 바꾼 값을 곱해서 하류로 보냅니다. 서로 바꾼 값이란 <code class="highlighter-rouge">Example 12</code> 처럼 순전파 때 $x$ 였다면 역전파에서는 $y$, 순전파 때 $y$ 였다면 역전파에서는 $x$ 로 바꾼다는 의미입니다.</p>

<p>구체적인 예를 들어보겠습니다. 가령 $10 \times 5 = 50$ 이라는 계산이 있고 역전파 때 상류에서 1.3 값이 흘러온닥 하겠습니다. 이를 계산 그래프로 그리면 <code class="highlighter-rouge">Example 13</code> 처럼 됩니다.</p>

<p>곱셈의 역전파는 순방향 입력 신호의 값이 필요합니다. 그래서 곱셈 노드를 구현할 때는 순전파의 입력 신호를 변수에 저장해 둡니다.</p>

<h2 id="implementing-a-simple-layer">Implementing a Simple Layer</h2>

<p>위에서 살펴본 사과 예를 Python 으로 구현해보겠습니다. 곱셈 노드를 <code class="highlighter-rouge">MulLayer</code> 덧셈 노드를 <code class="highlighter-rouge">AddLayer</code> 라는 이름으로 구현하겠습니다.</p>

<h3 id="multiplication-layer">Multiplication Layer</h3>

<p>모든 계층은 <code class="highlighter-rouge">forward()</code> 와 <code class="highlighter-rouge">backward()</code> 라는 공통의 메소드를 갖도록 구현할것입니다. <code class="highlighter-rouge">forward()</code> 는 순전파, <code class="highlighter-rouge">backward()</code> 는 역전파를 처리합니다.</p>

<p>먼저 곱셈 계층을 구현해보겠습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">MulLayer</span> <span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">x</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">y</span> <span class="o">=</span> <span class="bp">None</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">x</span> <span class="o">=</span> <span class="n">x</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">y</span> <span class="o">=</span> <span class="n">y</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">y</span>

        <span class="k">return</span> <span class="n">out</span>
    
    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dout</span><span class="p">):</span>
        <span class="n">dx</span> <span class="o">=</span> <span class="n">dout</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">y</span>
        <span class="n">dy</span> <span class="o">=</span> <span class="n">dout</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">x</span>

        <span class="k">return</span> <span class="n">dx</span><span class="p">,</span> <span class="n">dy</span>
</code></pre></div></div>

<p><code class="highlighter-rouge">__init__()</code> 에서는 인스턴스 변수 $x$ 와 $y$ 를 초기화합니다. 이 두 변수는 순전파 시의 입력 값을 유지하기 위해 사용합니다. <code class="highlighter-rouge">forward()</code> 에서는 $x$ 와 $y$ 를 인수로 받고 두 값을 곱해서 반환합니다. 반면 <code class="highlighter-rouge">backward()</code> 에서는 상류에서 넘어온 미분<code class="highlighter-rouge">(dout)</code>  에 순전파 때의 값을 서로바꿔 곱한 후 하류로 흘려보냅니다.</p>

<p>MulLayer 를 사용해서 앞에서 본 사과 쇼핑을 계산 그래프로 구현해보겠습니다.</p>

<p>MulLayer 를 사용하여 <code class="highlighter-rouge">Example 16</code> 의 순전파를 다음과 같이 구현할 수 있습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">apple</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">apple_num</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">tax</span> <span class="o">=</span> <span class="mf">1.1</span>

<span class="c1"># Layer
</span><span class="n">mul_apple_layer</span> <span class="o">=</span> <span class="n">MulLayer</span><span class="p">()</span>
<span class="n">mul_tax_layer</span> <span class="o">=</span> <span class="n">MulLayer</span><span class="p">()</span>

<span class="c1"># Forward
</span><span class="n">apple_price</span> <span class="o">=</span> <span class="n">mul_apple_layer</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">apple</span><span class="p">,</span> <span class="n">apple_num</span><span class="p">)</span>
<span class="n">price</span> <span class="o">=</span> <span class="n">mul_tax_layer</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">apple_price</span><span class="p">,</span> <span class="n">tax</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="n">price</span><span class="p">)</span>
</code></pre></div></div>

<p>각 변수에 대한 미분은 <code class="highlighter-rouge">backward()</code> 에서 구할 수 있습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">dprice</span><span class="o">=</span><span class="mi">1</span>
<span class="n">dapple_price</span><span class="p">,</span> <span class="n">dtax</span> <span class="o">=</span> <span class="n">mul_tax_layer</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">dprice</span><span class="p">)</span>
<span class="n">dapple</span><span class="p">,</span> <span class="n">dapple_num</span> <span class="o">=</span> <span class="n">mul_apple_layer</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">dapple_price</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="n">dapple</span><span class="p">,</span> <span class="n">dapple_num</span><span class="p">,</span> <span class="n">dtax</span><span class="p">)</span>
</code></pre></div></div>

<p><code class="highlighter-rouge">backward()</code> 호출 순서는 <code class="highlighter-rouge">forward()</code> 때와는 반대입니다. 또, <code class="highlighter-rouge">backward()</code> 가 받는 인수는 <em>순전파의 출력에 대한 미분</em> 입니다. 가령 <code class="highlighter-rouge">mul_apple_layer</code> 라는 곱셈 계층은 순전파때는 <code class="highlighter-rouge">apple_price</code> 를 출력합니다만, 역전파 때는 <code class="highlighter-rouge">apple_price</code> 의 미분 값인 <code class="highlighter-rouge">dapple_price</code> 를 인수로 받습니다. 이 코드를 실행한 결과는 <code class="highlighter-rouge">Example 16</code> 의 결과와 일치합니다.</p>

<h3 id="addition-layer">Addition Layer</h3>

<p>덧셈 노드의 계층은 다음과 같이 구현할 수 있습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">AddLayer</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">y</span>
        <span class="k">return</span> <span class="n">out</span>

    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dout</span><span class="p">):</span>
        <span class="n">dx</span> <span class="o">=</span> <span class="n">dout</span> <span class="o">*</span> <span class="mi">1</span>
        <span class="n">dy</span> <span class="o">=</span> <span class="n">dout</span> <span class="o">*</span> <span class="mi">1</span>
        <span class="k">return</span> <span class="n">dx</span><span class="p">,</span> <span class="n">dy</span>
</code></pre></div></div>

<p>덧셈 계층의 <code class="highlighter-rouge">forward()</code> 에서는 입력받은 두 인수 <code class="highlighter-rouge">x</code>, <code class="highlighter-rouge">y</code> 를 더해서 반환합니다. <code class="highlighter-rouge">backward()</code> 에서는 상류에서 내려온 미분을 그대로 하류로 흘립니다.</p>

<p>덧셈 계층과 곱셈 계층을 사용하여 사과 2 개와 귤 3 개를 사는 <code class="highlighter-rouge">Example 17</code> 의 상황을 구현해보겠습니다.</p>

<blockquote>
  <p>Example 17</p>
</blockquote>

<p><code class="highlighter-rouge">Example 17</code> 의 계산 그래프를 Python 으로 구현하면 다음과 같습니다. 코드는 <a href="https://github.com/WegraLee/deep-learning-from-scratch/blob/master/ch05/buy_apple_orange.py">Github</a> 에서 참조했습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">apple</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">apple_num</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">orange</span> <span class="o">=</span> <span class="mi">150</span>
<span class="n">orange_num</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">tax</span> <span class="o">=</span> <span class="mf">1.1</span>

<span class="c1"># layer
</span><span class="n">mul_apple_layer</span> <span class="o">=</span> <span class="n">MulLayer</span><span class="p">()</span>
<span class="n">mul_orange_layer</span> <span class="o">=</span> <span class="n">MulLayer</span><span class="p">()</span>
<span class="n">add_apple_orange_layer</span> <span class="o">=</span> <span class="n">AddLayer</span><span class="p">()</span>
<span class="n">mul_tax_layer</span> <span class="o">=</span> <span class="n">MulLayer</span><span class="p">()</span>

<span class="c1"># forward
</span><span class="n">apple_price</span> <span class="o">=</span> <span class="n">mul_apple_layer</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">apple</span><span class="p">,</span> <span class="n">apple_num</span><span class="p">)</span>  <span class="c1"># (1)
</span><span class="n">orange_price</span> <span class="o">=</span> <span class="n">mul_orange_layer</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">orange</span><span class="p">,</span> <span class="n">orange_num</span><span class="p">)</span>  <span class="c1"># (2)
</span><span class="n">all_price</span> <span class="o">=</span> <span class="n">add_apple_orange_layer</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">apple_price</span><span class="p">,</span> <span class="n">orange_price</span><span class="p">)</span>  <span class="c1"># (3)
</span><span class="n">price</span> <span class="o">=</span> <span class="n">mul_tax_layer</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">all_price</span><span class="p">,</span> <span class="n">tax</span><span class="p">)</span>  <span class="c1"># (4)
</span>
<span class="c1"># backward
</span><span class="n">dprice</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">dall_price</span><span class="p">,</span> <span class="n">dtax</span> <span class="o">=</span> <span class="n">mul_tax_layer</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">dprice</span><span class="p">)</span>  <span class="c1"># (4)
</span><span class="n">dapple_price</span><span class="p">,</span> <span class="n">dorange_price</span> <span class="o">=</span> <span class="n">add_apple_orange_layer</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">dall_price</span><span class="p">)</span>  <span class="c1"># (3)
</span><span class="n">dorange</span><span class="p">,</span> <span class="n">dorange_num</span> <span class="o">=</span> <span class="n">mul_orange_layer</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">dorange_price</span><span class="p">)</span>  <span class="c1"># (2)
</span><span class="n">dapple</span><span class="p">,</span> <span class="n">dapple_num</span> <span class="o">=</span> <span class="n">mul_apple_layer</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">dapple_price</span><span class="p">)</span>  <span class="c1"># (1)
</span>
<span class="k">print</span><span class="p">(</span><span class="s">"price:"</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">price</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="s">"dApple:"</span><span class="p">,</span> <span class="n">dapple</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"dApple_num:"</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">dapple_num</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="s">"dOrange:"</span><span class="p">,</span> <span class="n">dorange</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"dOrange_num:"</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">dorange_num</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="s">"dTax:"</span><span class="p">,</span> <span class="n">dtax</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="implement-activation-function-layer">Implement Activation Function Layer</h2>

<p>활성화 함수인 ReLU / Sigmoid Layer 를 구현해보겠습니다.</p>

<h3 id="relu-layer">ReLU Layer</h3>

<p>ReLU 함수에 사용되는 규칙은 다음과 같습니다.</p>

<blockquote>
  <p>Expression 7</p>
</blockquote>

<script type="math/tex; mode=display">y = \begin{cases}
x \; (x>0) \\
0 \; (x \leq 0 )
    
\end{cases}</script>

<p><code class="highlighter-rouge">Expression 7</code> 에서 $x$ 대한 $y$ 의 미분은 <code class="highlighter-rouge">Example 8</code> 처럼 구합니다.</p>

<blockquote>
  <p>Expression 8</p>
</blockquote>

<script type="math/tex; mode=display">{ \partial y \over \partial x} = \begin{cases}
1 \; (x>0) \\
0 \; (x \leq 0)
\end{cases}</script>

<p><code class="highlighter-rouge">Expression 8</code> 에서와 같이 순전파 때의 입력이 $x$ 가 0 보다 크면 역전파는 상류의 값을 그대로 하류로 흘려보냅니다. 하지만 순전파 때 $x$ 가 0 이면 역전파 대는 하류로 신호를 보내지 않습니다.</p>

<p>계산 그래프로는 <code class="highlighter-rouge">Example 18</code> 처럼 그릴 수 있습니다.</p>

<blockquote>
  <p>Example 18</p>
</blockquote>

<p>ReLU 를 Python 으로 구현해보겠습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">ReLU</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mask</span> <span class="o">=</span> <span class="bp">None</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mask</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="n">out</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">mask</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="k">return</span> <span class="n">out</span>

    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dout</span><span class="p">):</span>
        <span class="n">dout</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">mask</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">dx</span> <span class="o">=</span> <span class="n">dout</span>

        <span class="k">return</span> <span class="n">dx</span>
</code></pre></div></div>

<p>ReLU 클래스는 <code class="highlighter-rouge">mask</code> 라는 인스턴스 변수를 가집니다. <code class="highlighter-rouge">mask</code> 는 <code class="highlighter-rouge">True/False</code> 로 구성된 Numpy 배열로 순전파의 입력인 <code class="highlighter-rouge">x</code> 의 원소 값이 0 이하인 경우 True, 그 외는 False 로 유지합니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span> <span class="p">[[</span><span class="mf">1.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">]])</span>
<span class="o">&gt;&gt;&gt;</span> <span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="p">[[</span> <span class="mf">1.</span> <span class="o">-</span><span class="mf">0.5</span><span class="p">]</span>
 <span class="p">[</span><span class="o">-</span><span class="mf">2.</span>  <span class="mf">3.</span> <span class="p">]]</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">mask</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="k">print</span><span class="p">(</span><span class="n">mask</span><span class="p">)</span>
<span class="p">[[</span><span class="bp">False</span>  <span class="bp">True</span><span class="p">]</span>
 <span class="p">[</span><span class="bp">True</span>  <span class="bp">False</span><span class="p">]]</span>
</code></pre></div></div>

<h3 id="sigmoid-layer">Sigmoid Layer</h3>

<p>아래 <code class="highlighter-rouge">Expression 9</code> 은 Sigmoid 함수를 의미하는 함수입니다.</p>

<blockquote>
  <p>Expression 9</p>
</blockquote>

<script type="math/tex; mode=display">y = { 1 \over 1 + \exp(-x)}</script>

<p><code class="highlighter-rouge">Expression 9</code> 를 계산 그래프로 그리면 <code class="highlighter-rouge">Example 19</code> 처럼 됩니다.</p>

<blockquote>
  <p>Example 19</p>
</blockquote>

<p><code class="highlighter-rouge">Example 19</code> 에서  $\times$ 와 $+$ 노드 말고도 $exp$ 와 $/$ 노드가 등장했습니다. $exp$ 노드는 $y = exp(x)$ 계산을 수행하고 $/$ 노드는 $y = { 1 \over x}$ 계산을 수행합니다.</p>

<p><code class="highlighter-rouge">Example 19</code> 와 같이 <code class="highlighter-rouge">Example 9</code> 의 계산은 국소적 계산의 전파로 이루어집니다. 이제 <code class="highlighter-rouge">Example 19</code> 의 역전파를 알아보겠습니다. 여기서 역전파의 흐름을 오른쪽에서 왼쪽으로 1 단계씩 알아보겠습니다.</p>

<h4 id="1-step">1 Step</h4>

<p>$/$ 노드, $y = { 1 \over x}$ 를 미분하면 다음 식이 됩니다.</p>

<blockquote>
  <p>Expression 10</p>
</blockquote>

<script type="math/tex; mode=display">\begin{align}
{ \partial y \over \partial x } = - { 1 \over x^2 } \\
= -y^2
\end{align}</script>

<p><code class="highlighter-rouge">Expression 10</code> 에 따르면 역전파 때는 상류에서 흘러온 값에 $-y^2$ 을 곱해서 하류로 전달합니다. 계산 그래프에서는 다음과 같습니다.</p>

<blockquote>
  <p>Example 19-1</p>
</blockquote>

<h4 id="2-step">2 Step</h4>

<p>$+$ 노드는 상류의 값을 여과없이 하류로 내보내는 게 다입니다. 계산 그래프에서는 다음과 같습니다.</p>

<blockquote>
  <p>Example 19-2</p>
</blockquote>

<h4 id="3-step">3 Step</h4>

<p>$exp$ 노드는 $y=exp(x)$ 연산을 수행하며, 그 마분은 다음과 같습니다.</p>

<blockquote>
  <p>Expression 11</p>
</blockquote>

<script type="math/tex; mode=display">{ \partial y \over \partial x} = \exp(x)</script>

<p>계산 그래프에서는 상류의 값에 순전파 때의 출력을 곱해 하류로 전파합니다.</p>

<blockquote>
  <p>Example 19-3</p>
</blockquote>

<h4 id="4-step">4 Step</h4>

<p>$\times$ 노드는 순전파 때의 값을 서로 바꿔서 곱합니다. 이 예에서는 $-1$ 을 곱합니다.</p>

<blockquote>
  <p>Example 20</p>
</blockquote>

<p><code class="highlighter-rouge">Example 20</code> 에서 알 수 있듯이 역전파의 최종 출력인 ${ \partial L \over \partial y}y^2 \exp(-x)$ 의 값이 하류 노드로 전파됩니다. 저 값은 순전파의 입력 $x$ 와 $y$ 만으로 계산할 수 있습니다.</p>

<p><code class="highlighter-rouge">Example 20</code> 의 계산 그래프의 중간 과정을 묶어 <code class="highlighter-rouge">Example 21</code> 처럼 간단하게 단순한 <code class="highlighter-rouge">sigmmoid</code> 노드 하나로 대체할 수 있습니다.</p>

<p>${ \partial L \over \partial y}y^2 \exp(-x)$ 는 다음처럼 정리해서 사용할 수 있습니다.</p>

<blockquote>
  <p>Expression 12</p>
</blockquote>

<script type="math/tex; mode=display">\begin{align}
= { \partial L \over \partial y} { 1 \over (1 + \exp(-x))^2} \exp(-x) \\
= { \partial L \over \partial y} { 1 \over 1 + \exp(-x)} { \exp(-x) \over 1 + \exp(-x)} \\
= { \partial L \over \partial y} y (1-y) 
\end{align}</script>

<p>이처럼 Sigmoid 계층의 역전파는 순전파의 출력 $(y)$ 만으로 계산할 수 있습니다.</p>

<blockquote>
  <p>Example 22</p>
</blockquote>

<p>Sigmoid 계층을 Python 으로 구현해보겠습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Sigmoid</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out</span> <span class="o">=</span> <span class="bp">None</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">out</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out</span> <span class="o">=</span> <span class="n">out</span>

        <span class="k">return</span> <span class="n">out</span>

    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dout</span><span class="p">):</span>
        <span class="n">dx</span> <span class="o">=</span> <span class="n">dout</span> <span class="o">*</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">out</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">out</span>
        
        <span class="k">return</span> <span class="n">dx</span>
</code></pre></div></div>

<h2 id="implementing-the-affine--softmax-layer">Implementing the Affine / Softmax Layer</h2>

<h3 id="affine-layer">Affine Layer</h3>

<p>행렬의 곱계산은 <code class="highlighter-rouge">Example 23</code> 처럼 차원의 원소 수를 일치시키는게 핵심입니다.</p>

<blockquote>
  <p>Example 23</p>
</blockquote>

<p>신경망의 순전파 때 수행하는 행렬의 곱은 기하학에서는 <strong>어파인 변환(Affine Transformation)</strong> 이라고 합니다. 그래서 이 책에서는 어파인 변환을 수행하는 처리를 Affine 계층이라는 이름으로 구현합니다.</p>

<p><code class="highlighter-rouge">Example 23</code> 수행한 연산을 계산 그래프로 그려보겠습니다. 곱을 계산하는 노드를 <code class="highlighter-rouge">dot</code> 이라 하면, <code class="highlighter-rouge">np.dot(X, W) + B</code> 계산은 <code class="highlighter-rouge">Example 24</code> 처럼 그립니다. 각 변수의 이름 위에 그 변수의 형상도 표기합니다. <code class="highlighter-rouge">Example 24</code> 에서는 $X$ 의 형상은 $(2, )$ $X \cdot W$ 의 형상은 $(3, )$ 임을 표기했습니다.</p>

<blockquote>
  <p>Example 24</p>
</blockquote>

<p><code class="highlighter-rouge">Example 24</code> 의 역전파에 대해 생각해보겠습니다. 행렬을 사용한 역전파도 행렬의 원소마다 전개해보면 스칼라값을 사용한 지금까지의 계산 그래프와 순서로 생각할 수 있습니다. 그 결과 다음 식이 도출됩니다.</p>

<blockquote>
  <p>Expression 13</p>
</blockquote>

<script type="math/tex; mode=display">{ \partial L \over \partial X } = { \partial L \over \partial Y } \cdot W^T \\
{ \partial L \over \partial W } = X^T \cdot { \partial L \over \partial Y }</script>

<p><code class="highlighter-rouge">Expression 13</code> 에서 $W^T$ 의 $T$ 전치 행렬을 뜻합니다. 전치행렬은 $W$ 의 $(i, j)$ 위치의 원소를 $(j, i)$ 위치로 바꾼겁니다. 수식으로는 아래와 같이 사용할 수 있습니다.</p>

<blockquote>
  <p>Expression 14</p>
</blockquote>

<script type="math/tex; mode=display">W = \begin{pmatrix}
w_{11} \; w_{12} \; w_{13} \\ 
w_{21} \; w_{22} \; w_{23}
\end{pmatrix} \\

W^T = \begin{bmatrix}
w_{11} \; w_{12} \\
w_{12} \; w_{22} \\
w_{13} \; w_{23}
\end{bmatrix}</script>

<p><code class="highlighter-rouge">Expression 14</code> 와 같이 $W$ 의 형상이 $(2, 3)$ 이였다면 전치행렬 $W^T$ 의 형상은 $(3, 2)$ 가 됩니다. <code class="highlighter-rouge">Expression 13</code> 을 바탕으로 계산 그래프의 역전파를 그려보겠습니다.</p>

<blockquote>
  <p>Example 25</p>
</blockquote>

<p><code class="highlighter-rouge">Example 25</code> 의 계산 그래프에서는 각 변수의 형상에 주의해서 살펴봅시다. 특히 $X$ 와 ${ \partial L \over \partial X }$ 은 같은 형상이고 $W$ 와 ${ \partial L \over \partial W }$ 도 같은 형상입니다. 이유는 아래 식을보면 됩니다.</p>

<blockquote>
  <p>Expression 15</p>
</blockquote>

<script type="math/tex; mode=display">X = (x_0, x_1, \cdot \cdot \cdot, x_n) \\
{ \partial L \over \partial X } = ({ \partial L \over \partial x_0 },
{ \partial L \over \partial x_1 }, \cdot \cdot \cdot,
{ \partial L \over \partial x_n })</script>

<p>행렬끼리 곱셈을 할 때에는 아래 그림과 같이 항상 형상의 주의해야합니다.</p>

<blockquote>
  <p>Example 26</p>
</blockquote>

<h3 id="affine-layer-for-batch">Affine Layer for Batch</h3>

<p>위에서 살펴본 Affine 계층은 입력 데이터 $X$ 하나만 가지고 고려했습니다. 이번에는 데이터 N 개를 모아 순전파하는, 배치용 Affine 계층을 생각해보겠습니다.</p>

<p>배치용 Affine 계층을 계산 그래프로 그려보겠습니다.</p>

<blockquote>
  <p>Example 27</p>
</blockquote>

<p>기존과 다른건 X 의 형상이 $(N, 2)$ 가 된것입니다.</p>

<p>편향을 더할 때도 주의해야 합니다. 순전파 때의 편향 덧셈은 $X \cdot W$ 에 대한 편향이 각 데이터에 더해집니다. 예를 들어 $N = 2$ 인 경우, 편향은 그 두 데이터 각각에 더해집니다. 구체적인 예를 보겠습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="n">X_dot_W</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,]])</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">B</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="o">&gt;&gt;&gt;</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">X_dot_W</span>
<span class="n">array</span><span class="p">([[</span> <span class="mi">0</span><span class="p">,</span>  <span class="mi">0</span><span class="p">,</span>  <span class="mi">0</span><span class="p">],</span>
       <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">]])</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">X_dot_W</span> <span class="o">+</span> <span class="n">B</span>
<span class="n">array</span><span class="p">([[</span> <span class="mi">1</span><span class="p">,</span>  <span class="mi">2</span><span class="p">,</span>  <span class="mi">3</span><span class="p">],</span>
       <span class="p">[</span><span class="mi">11</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">13</span><span class="p">]])</span>
</code></pre></div></div>

<p>역전파 때는 각 데이터의 역전파 값이 편향의 원소에 모여야 합니다. 코드로는 다음과 같습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="n">dY</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]])</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">dY</span>
<span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
       <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]])</span>
<span class="o">&gt;&gt;&gt;</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">dB</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">dY</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">dB</span>
<span class="n">array</span><span class="p">([</span><span class="mi">5</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">9</span><span class="p">])</span>
</code></pre></div></div>

<p>Affine 을 코드로 구현하면 아래와 같습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="k">class</span> <span class="nc">Affine</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">W</span> <span class="o">=</span> <span class="n">W</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">b</span> <span class="o">=</span> <span class="n">b</span>        
        <span class="bp">self</span><span class="o">.</span><span class="n">x</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dW</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">db</span> <span class="o">=</span> <span class="bp">None</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">x</span> <span class="o">=</span> <span class="n">x</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">b</span>

        <span class="k">return</span> <span class="n">out</span>

    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dout</span><span class="p">):</span>
        <span class="n">dx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">dout</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dW</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">x</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">dout</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">db</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">dout</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">dx</span>
</code></pre></div></div>

<h3 id="softmax-with-loss-layer">Softmax-with-Loss Layer</h3>

<p>소프트맥스 함수는 입력 값을 정규화하여 출력합니다. 예를 들어 MNIST 데이터 인식 Softmax 계층의 출력은 <code class="highlighter-rouge">Example 28</code> 처럼 됩니다.</p>

<blockquote>
  <p>Example 28</p>
</blockquote>

<p>Softmax 계층은 입력 값을 정규화하여 출력합니다. MNIST 데이터가 10 개 이므로 Softmax 계층의 입력은 10개가 됩니다.</p>

<p>Softmax 계층은 손실함수인 교차 엔트로피 오차도 포함하여 <em>Softmax-with-Loss Layer</em> 이라고도 합니다.</p>

<blockquote>
  <p>Example 29</p>
</blockquote>

<p>Softmax-with-Loss 계층은 복잡해서 간단한 계산그래프로 나타내면 <code class="highlighter-rouge">Example 30</code> 처럼 나타낼 수 있습니다.</p>

<blockquote>
  <p>Example 30</p>
</blockquote>

<p><code class="highlighter-rouge">Example 30</code> 에서 주목할만한 것은 역전파의 결과입니다. Softmax 계층의 역전파는 $(y_1 - t_1, y_2 - t_2, y_3 - t_3)$ 라는 깔끔한 결과를 출력합니다. $(y_1, y_2, y_3)$ 는 Softmax 계층의 출력이고 $(t_1, t_2, t_3)$ 는 정답 레이블이므로 $(y_1 - t_1, y_2 - t_2, y_3 - t_3)$ 는 Softmax 계층의 출력과 정답 레이블의 차분입니다. 신경망의 역전파에서는 이 차이인 오차가 앞 계층에 전해지는 것입니다. 이는 신경망 학습의 중요한 성질입니다.</p>

<p>Softmax-with-Loss 계층의 코드를 보겠습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">SoftmaxWithLoss</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">loss</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">y</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">t</span> <span class="o">=</span> <span class="bp">None</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">t</span> <span class="o">=</span> <span class="n">t</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">y</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">loss</span> <span class="o">=</span> <span class="n">cross_entropy_error</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">y</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">t</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span>

    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dout</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">t</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">dx</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">y</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">t</span><span class="p">)</span> <span class="o">/</span> <span class="n">batch_size</span>
        <span class="k">return</span> <span class="n">dx</span>
</code></pre></div></div>

<h2 id="implementation-of-backpropagation">Implementation of Backpropagation</h2>

<h3 id="implementing-a-neural-network-using-backpropagation">Implementing a Neural Network Using Backpropagation</h3>

<p>2 층 신경망을 <code class="highlighter-rouge">TwoLayerNet</code> 클래스로 구현해보겠습니다. 먼저 이 클래스의 인스턴스 변수와 메서드를 정리한 표를 확인하면 됩니다.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: left">Instance Variable</th>
      <th style="text-align: left">Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">params</td>
      <td style="text-align: left">딕셔너리 변수로, 신경망의 매개변수를 보관</td>
    </tr>
    <tr>
      <td style="text-align: left">layers</td>
      <td style="text-align: left">순서가 있는 딕셔너리 변수로, 신경망의 계층을 보관</td>
    </tr>
    <tr>
      <td style="text-align: left">lastLayer</td>
      <td style="text-align: left">신경망의 마지막 계층, SoftmaxWithLoss 계층</td>
    </tr>
  </tbody>
</table>

<table>
  <thead>
    <tr>
      <th style="text-align: left">Method</th>
      <th style="text-align: left">Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left"><strong>init</strong>(self, input_size, hidden_size, output_size, weight_init_std)</td>
      <td style="text-align: left">초기화를 수행한다. 인수는 맨 앞에서부터 입력층 뉴런의 수, 은닉층 뉴런 수, 출력층 뉴런 수, 가중치 초기화 시 정규분포의 스케일</td>
    </tr>
    <tr>
      <td style="text-align: left">predict(self, x)</td>
      <td style="text-align: left">예측을 수행, x 는 이미지 데이터</td>
    </tr>
    <tr>
      <td style="text-align: left">loss(self, x, t)</td>
      <td style="text-align: left">손실 함수의 값을 구한다. x 는 이미지 데이터, t 는 정답 레이블</td>
    </tr>
    <tr>
      <td style="text-align: left">accuracy(self, x, t)</td>
      <td style="text-align: left">정확도를 구한다.</td>
    </tr>
    <tr>
      <td style="text-align: left">numerical_gradient(self, x, t)</td>
      <td style="text-align: left">가중치 매개변수의 기울기를 수치 미분 방식으로 구한다.</td>
    </tr>
    <tr>
      <td style="text-align: left">gradient(self, x, t)</td>
      <td style="text-align: left">가중치 매개변수의 기울기를 오차역전파법으로 구한다.</td>
    </tr>
  </tbody>
</table>

<p>코드로 알아보겠습니다. 전체 코드는<a href="https://github.com/WegraLee/deep-learning-from-scratch/blob/master/ch05/two_layer_net.py">Github</a> 에 있습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">TwoLayerNet</span><span class="p">:</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="n">weight_init_std</span> <span class="o">=</span> <span class="mf">0.01</span><span class="p">):</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">params</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s">'W1'</span><span class="p">]</span> <span class="o">=</span> <span class="n">weight_init_std</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s">'b1'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s">'W2'</span><span class="p">]</span> <span class="o">=</span> <span class="n">weight_init_std</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">)</span> 
        <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s">'b2'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">output_size</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">layers</span> <span class="o">=</span> <span class="n">OrderedDict</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="s">'Affine1'</span><span class="p">]</span> <span class="o">=</span> <span class="n">Affine</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s">'W1'</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s">'b1'</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="s">'Relu1'</span><span class="p">]</span> <span class="o">=</span> <span class="n">Relu</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="s">'Affine2'</span><span class="p">]</span> <span class="o">=</span> <span class="n">Affine</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s">'W2'</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s">'b2'</span><span class="p">])</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">lastLayer</span> <span class="o">=</span> <span class="n">SoftmaxWithLoss</span><span class="p">()</span>
</code></pre></div></div>

<h3 id="verification-of-gradient-with-backpropagation">Verification of Gradient with Backpropagation</h3>

<p>코드는 <a href="https://github.com/WegraLee/deep-learning-from-scratch/blob/master/ch05/gradient_check.py">Github</a> 에서 확인하면됩니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">mport</span> <span class="n">sys</span><span class="p">,</span> <span class="n">os</span>
<span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">pardir</span><span class="p">)</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="nn">dataset.mnist</span> <span class="kn">import</span> <span class="n">load_mnist</span>
<span class="kn">from</span> <span class="nn">two_layer_net</span> <span class="kn">import</span> <span class="n">TwoLayerNet</span>

<span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">t_train</span><span class="p">),</span> <span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">t_test</span><span class="p">)</span> <span class="o">=</span> <span class="n">load_mnist</span><span class="p">(</span><span class="n">normalize</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">one_hot_label</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">network</span> <span class="o">=</span> <span class="n">TwoLayerNet</span><span class="p">(</span><span class="n">input_size</span><span class="o">=</span><span class="mi">784</span><span class="p">,</span> <span class="n">hidden_size</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">output_size</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

<span class="n">x_batch</span> <span class="o">=</span> <span class="n">x_train</span><span class="p">[:</span><span class="mi">3</span><span class="p">]</span>
<span class="n">t_batch</span> <span class="o">=</span> <span class="n">t_train</span><span class="p">[:</span><span class="mi">3</span><span class="p">]</span>

<span class="n">grad_numerical</span> <span class="o">=</span> <span class="n">network</span><span class="o">.</span><span class="n">numerical_gradient</span><span class="p">(</span><span class="n">x_batch</span><span class="p">,</span> <span class="n">t_batch</span><span class="p">)</span>
<span class="n">grad_backprop</span> <span class="o">=</span> <span class="n">network</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">x_batch</span><span class="p">,</span> <span class="n">t_batch</span><span class="p">)</span>

<span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">grad_numerical</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
    <span class="n">diff</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">average</span><span class="p">(</span> <span class="n">np</span><span class="o">.</span><span class="nb">abs</span><span class="p">(</span><span class="n">grad_backprop</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">-</span> <span class="n">grad_numerical</span><span class="p">[</span><span class="n">key</span><span class="p">])</span> <span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="n">key</span> <span class="o">+</span> <span class="s">":"</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">diff</span><span class="p">)</span>
</code></pre></div></div>

<p>MNIST 데이터 셋을 읽어서 훈련 데이터 일부를 수치 미분으로 구한 기울기와 오차역전파법으로 구한 기울기의 오차를 확인해보겠습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">b1</span><span class="p">:</span><span class="mf">9.70418809871e-13</span>
<span class="n">W2</span><span class="p">:</span><span class="mf">8.41139039497e-13</span>
<span class="n">b2</span><span class="p">:</span><span class="mf">1.1945999745e-10</span>
<span class="n">W1</span><span class="p">:</span><span class="mf">2.2232446644e-13</span>
</code></pre></div></div>

<p>결과를 살펴보면 수치 미분과 오차역전파법으로 구한 기울기의 차이가 매우 적다는것을 알 수 있습니다.</p>

<h3 id="to-implement-learning-using-backpropagation">To Implement Learning Using Backpropagation</h3>

<p>마지막으로 오차역전파법을 사용한 신경망을 구현하겠습니다. 소스 코드는 <a href="https://github.com/WegraLee/deep-learning-from-scratch/blob/master/ch05/train_neuralnet.py">Github</a> 에 있습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">sys</span><span class="p">,</span> <span class="n">os</span>
<span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">pardir</span><span class="p">)</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="nn">dataset.mnist</span> <span class="kn">import</span> <span class="n">load_mnist</span>
<span class="kn">from</span> <span class="nn">two_layer_net</span> <span class="kn">import</span> <span class="n">TwoLayerNet</span>

<span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">t_train</span><span class="p">),</span> <span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">t_test</span><span class="p">)</span> <span class="o">=</span> <span class="n">load_mnist</span><span class="p">(</span><span class="n">normalize</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">one_hot_label</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">network</span> <span class="o">=</span> <span class="n">TwoLayerNet</span><span class="p">(</span><span class="n">input_size</span><span class="o">=</span><span class="mi">784</span><span class="p">,</span> <span class="n">hidden_size</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">output_size</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

<span class="n">iters_num</span> <span class="o">=</span> <span class="mi">10000</span>
<span class="n">train_size</span> <span class="o">=</span> <span class="n">x_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.1</span>

<span class="n">train_loss_list</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">train_acc_list</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">test_acc_list</span> <span class="o">=</span> <span class="p">[]</span>

<span class="n">iter_per_epoch</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">train_size</span> <span class="o">/</span> <span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">iters_num</span><span class="p">):</span>
    <span class="n">batch_mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">train_size</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>
    <span class="n">x_batch</span> <span class="o">=</span> <span class="n">x_train</span><span class="p">[</span><span class="n">batch_mask</span><span class="p">]</span>
    <span class="n">t_batch</span> <span class="o">=</span> <span class="n">t_train</span><span class="p">[</span><span class="n">batch_mask</span><span class="p">]</span>
    
    <span class="n">grad</span> <span class="o">=</span> <span class="n">network</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">x_batch</span><span class="p">,</span> <span class="n">t_batch</span><span class="p">)</span> 
    
    <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="p">(</span><span class="s">'W1'</span><span class="p">,</span> <span class="s">'b1'</span><span class="p">,</span> <span class="s">'W2'</span><span class="p">,</span> <span class="s">'b2'</span><span class="p">):</span>
        <span class="n">network</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">-=</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">grad</span><span class="p">[</span><span class="n">key</span><span class="p">]</span>
    
    <span class="n">loss</span> <span class="o">=</span> <span class="n">network</span><span class="o">.</span><span class="n">loss</span><span class="p">(</span><span class="n">x_batch</span><span class="p">,</span> <span class="n">t_batch</span><span class="p">)</span>
    <span class="n">train_loss_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
    
    <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="n">iter_per_epoch</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">train_acc</span> <span class="o">=</span> <span class="n">network</span><span class="o">.</span><span class="n">accuracy</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">t_train</span><span class="p">)</span>
        <span class="n">test_acc</span> <span class="o">=</span> <span class="n">network</span><span class="o">.</span><span class="n">accuracy</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">t_test</span><span class="p">)</span>
        <span class="n">train_acc_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">train_acc</span><span class="p">)</span>
        <span class="n">test_acc_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">test_acc</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="n">train_acc</span><span class="p">,</span> <span class="n">test_acc</span><span class="p">)</span>
</code></pre></div></div>
:ET