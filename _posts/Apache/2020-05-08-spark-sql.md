---
title : Spark SQL
tags :
- Catalog
- Database
- Table Metadata
- Table
- View
- Spark SQL
- Spark
---

*이 포스트는 [Spark The Definitive Guide](https://github.com/manparvesh/bigdata-books/blob/master/Spark%20-%20The%20Definitive%20Guide%20-%20Big%20data%20processing%20made%20simple.pdf) 를 바탕으로 작성하였습니다.*

Spark SQL 은 Spark 에서 중요하고 강력한 기능 중 하나입니다.

Spark SQL 을 사용해 데이터베이스에 생성된 **뷰(View)** 나 테이블에 SQL 쿼리를 실행할 수 있습니다. 또한 시스템 함수를 사용하거나 사용자 정의 함수를 할정의할 수 있습니다. 워크로드를 최적화하기 위해 쿼리 실행 계획을 분석할 수 있습니다.

Spark SQL 은 DataFrame 과 Dataset API 에 통합되어 있습니다. 따라서 데이터 변환 시 SQL 과 DataFrame 의 기능을 모두 사용할 수 있으며 두 방식 모두 동일한 실행 코드로 컴파일 됩니다.

## What Is SQL?

SQL 또는 **구조적 질의 언어(Structured Query Language)** 는 데이터에 대한 관계형 연산을 표현하기 위한 도메인 특화 언어입니다. 모든 RDB 에서 사용되며, NoSQL 데이터베이스에서도 사용할 수 있는 변형된 자체 SQL 을 제공합니다.

## Big Data and SQL: Apache Hive

Spark 가 등장하기 전에는 Hive 가 빅데이터 SQL 접근 계층에서 사리상 표준이였습니다. Hive 로 SQL 쿼리를 실행할 수 있게 되며 Hadoop 을 다양한 산업군으로 진출시켰습니다.

Spark 는 RDD 를 이용한 범용 처리 엔진으로 시작했지만 이제는 많은 사용자가 Spark SQL 을 사용합니다.

## Big Data and SQL: Spark SQL

Spark 2.0 버전에서 Hive 는 지원할 수 있는 상위 호환 기능으로 ANSI-SQL 과 HiveQL 을 모두 지원하는 자체 개발된 SQL 파서가 포함되어 있습니다. Spark SQL 은 DataFrame 과의 뛰어난 호환성 덕분에 다양한 기업에서 강력한 기능으로 자리잡습니다.

SQL 분석가들은 **쓰리프트 서버(Thrift Server)** 나 SQL 인터페이스에 접속해 Spark 의 연산 능력을 활용할 수 있습니다. 그리고 데이터 엔지니어와 과학자는 전체 데이터 처리 파이프라인에 Spark SQL 을 사용할 수 있습니다.

이 통합형 API 는 SQL 로 데이터를 조회하고 DataFrame 으로 변환한 다음 Spark 의 MLlib 이 제공하는 대규모 머신러닝 알고리즘 중 하나를 사용해 수행한 결과를 다른 데이터소스에 저장하는 전체 과정을 가능하게 만듭니다.

> Spark SQL 은 **온라인 트랜잭션 처리(Online Transaction Processing, OLTP)** 데이터베이스가 아닌 **온라인 분석용(Online Analytic Processing, OLAP)** 데이터베이스로 동작합니다. 즉, 매우 낮은 지연 시간이 필요한 쿼리를 수행하기 위한 용도로 사용할 수 없습니다. 

### Spark’s Relationship to Hive

Spark SQL 은 Hive 메타스토어를 사용하므로 Hive 와 잘 연동할 수 있습니다. Hive 메타스토어는 여러 세션에서 사용할 테이블 정보를 보관하고 있습니다. Spark SQL 은 Hive 메타스토어에 접속한 뒤 조회할 파일 수를 최소화하기 위해 메타데이터를 참조합니다.

이 기능은 기존 Hadoop 환경의 모든 워크로드를 Spark 로 이관하려는 사용자들에게 인기 있습니다.

#### THE HIVE METASTORE

Hive 메타스토어에 접속하려면 몇 가지 속성이 필요합니다. 먼저 접근하려는 Hive 메타스토어에 적합한 버전은 *spark.sql.hive.metastore.version* 에 설정합니다. 기본값은 1.2.1 입니다.

또한 HiveMetastoreClient 가 초기화하는 방식을 변경하려면 *spark.sql.hive.metastore.jars* 를 설정합니다. Spark 는 기본 버전을 사용하지만 Maven 저장소나 JVM 의 표준 포맷에 맞게 클래스패스에 정의할 수 있습니다. 또한 Hive 메타스토어가 저장된 다른 데이터베이스에 접속하려면 적합한 클래스 접두사를 정의해야 합니다. Spark 와 Hive 에서 공유할 수 있도록 클래스 접두사를 *spark.sql.hive.metastore.sharedPrefixes* 속성에 설정합니다.

## How to Run Spark SQL Queries

Spark SQL 쿼리를 실행할 수 있는 몇 가지 인터페이스를 제공합니다.

### Spark SQL CLI

Spark SQL CLI 는 로컬 환경의 명령행에서 기본 Spark SQL 쿼리를 실행할 수 있는 편리한 도구입니다. Spark SQL CLI 는 쓰리프트 JDBC 서버와 통신할 수 없습니다. Spark SQL CLI 를 사용하려면 Spark 디렉토리에서 다음 명령을 실행합니다.

```shell
./bin/spark-sql
```

Spark 가 설치된 경로의 *conf* 디렉토리에 *hive-site.xml*, *core-site.xml*, *hdfs-site.xml* 파일을 배치해 Hive 를 사용할 수 있는 환경을 구성할 수 있습니다. 사용 가능한 전체 옵션을 보려면 `./bin/spark-sql --help` 를 실행합니다.

### Spark’s Programmatic SQL Interface

서버를 설정해 SQL 을 사용할 수 있지만, Spark 에서 지원하는 언어 API 로 비정형 SQL 을 실행할 수 있습니다. 이를 위해 SparkSession 객체의 `sql` 메소드를 사용합니다. 처리된 결과는 DataFrame 을 반환합니다. 예를 들어 Python 이나 Scala 에서 다음과 같은 코드를 실행할 수 있습니다.

```scala
spark.sql("SELECT 1+1").show()
```

`spark.sql("SELECT 1+1")` 명령은 프로그래밍 방식으로 평가할 수 있는 DataFrame 을 반환합니다. 다른 트랜스포메이션과 마찬가지로 즉시 실행되지 않고 지연 처리됩니다. 또한, DataFrame 을 사용하는 것보다 SQL 코드로 표현하기 쉬운 트랜스포메이션이기 때문에 강력한 인터페이스입니다.

함수에 여러 줄로 구성된 문자열을 전달할 수 있으므로 여러 줄로 구성된 쿼리를 아주 간단히 표현할 수 있습니다. 예를 들어 Python 이나 Scala 에서 다음과 같은 코드를 실행할 수 있습니다.

```scala
spark.sql("""SELECT user_id, department, first_name FROM professors
  WHERE department IN
    (SELECT name FROM department WHERE created_date >= '2016-01-01')""")
```

심지어 SQL 과 DataFrame 은 완벽하게 연동될 수 있으므로 더 강력합니다. 예를 들어 DataFrame 을 생성하고 SQL 을 사용해 처리할 수 있으며 그 결과를 다시 DataFrame 으로 돌려받게 됩니다. 이 바익은 다양한 처리에 자주 사용하게 될 매우 효과적인 패턴 중 하나입니다.

```scala
// in Scala
spark.read.json("/data/flight-data/json/2015-summary.json")
  .createOrReplaceTempView("some_sql_view") // DF => SQL

spark.sql("""
SELECT DEST_COUNTRY_NAME, sum(count)
FROM some_sql_view GROUP BY DEST_COUNTRY_NAME
""")
  .where("DEST_COUNTRY_NAME like 'S%'").where("`sum(count)` > 10")
  .count() // SQL => DF
```

### SparkSQL Thrift JDBC/ODBC Server

Spark 는 **자바 데이터베이스 연결(Java Database Connectivity, JDBC)** 인터페이스를 제공합니다. 사용자나 원격 프로그램은 Spark SQL 을 실행하기 위해 이 인터페이스로 Spark 드라이버에 접속합니다. 

쓰리프트 JDBC / ODBC(Open Database Connectivity) 서버는 Hive 1.2.1 버전의 HiveServer2 에 맞추어 구현되어 있습니다. Spark 나 Hive 1.2.1 버전에 있는 `beeline` 스크립트를 이용해 JDBC 서버를 테스트해볼 수 있습니다. JDBC / ODBC 서버를 시작하려면 Spark 디렉토리에서 다음 명령을 실행합니다.

```shell
./sbin/start-thriftserver.sh
```

이 스크립트는 `bin/spark-submit` 스크립트에서 사용할 수 있는 모든 명령행 옵션을 지원합니다. 쓰리프트 서버의 전체 설정 옵션을 확인하려면 `./sbin/start-thriftserver.sh --help` 명령을 실행합니다. 쓰리프트 서버는 기본적으로 localhost:10000 주소를 사용합니다. 환경 변수나 시스템 속성을 지정해 쓰리프트 서버의 주소를 변경할 수 있습니다.

환경 변수는 다음과 같이 설정합니다.

```shell
export HIVE_SERVER2_THRIFT_PORT=<listening-port>
export HIVE_SERVER2_THRIFT_BIND_HOST=<listening-host>
./sbin/start-thriftserver.sh \
  --master <master-uri> \
  ...
```

시스템 속성은 다음과 같이 설정합니다.

```shell
./sbin/start-thriftserver.sh \
  --hiveconf hive.server2.thrift.port=<listening-port> \
  --hiveconf hive.server2.thrift.bind.host=<listening-host> \
  --master <master-uri>
  ...
```

서버가 시작되면 다음 명령을 사용해 접속 테스트를 합니다.

```shell
./bin/beeline
beeline> !connect jdbc:hive2://localhost:10000
```

`beeline` 은 사용자 이름과 비밀번호를 요구합니다. 비보안 모드의 경우에는 단순히 로컬 사용자 이름을 입력하며 비밀번호는 입력하지 않아도 됩니다. 보안 모드의 경우에는 `beeline` 문서에서 제시하는 방법을 따라야 합니다.

## Catalog

Spark SQL 에 가장 높은 추상화 단계는 **카탈로그(Catalog)** 입니다. 카탈로그는 테이블에 저장된 데이터에 대한 메타데이터 뿐만 아니라 데이터베이스, 테이블, 함수, 뷰에 대한 정보를 추상화합니다. 카탈로그는 *org.apache.spark.sql.catalog.Catalog* 패키지로 사용할 수 있습니다.

카탈로그는 테이블, 데이터베이스 그리고 여러가지 유용한 함수를 제공합니다.

## Tables

Spark SQL 을 사용해 유용한 작업을 수행하려면 먼저 테이블을 정의해야 합니다. 테이블은 명령을 실행할 데이터의 구조라는 점에서 DataFrame 과 논리적으로 동일합니다. 테이블은 조인, 필터링, 집계등 여러 데이터 변환 작업을 수행할 수 있습니다.

DataFrame 은 프로그래밍 언어로 정의하지만 테이블은 데이터베이스에서 정의합니다. Spark 에서 테이블을 생성하면 default 데이터베이스에 등록됩니다.

Spark 2.x 버전에서는 테이블은 항상 데이터를 가지고 있습니다. 임시 테이블의 개념이 없으며 데이터를 가지지 않은 뷰만 존재합니다. 테이블을 제거하면 모든 데이터가 제거되므로 조심해야 합니다.

### Spark-Managed Tables

**관리형 테이블** 과 **외부 테이블** 의 개념을 반드시 기억해야 합니다. 테이블은 두 가지 중요한 정보를 저장합니다. 테이블의 데이터와 테이블에 대한 데이터, 즉 **메타데이터** 입니다. Spark 는 데이터뿐만 아니라 파일에 대한 메타데이터를 관리할 수 있습니다. 디스크에 저장된 파일을 이용해 테이블을 정의하면 외부 테이블을 정의하는 겁니다. DataFrame 의 `saveAsTable` 메소드는 Spark 가 관련된 모든 정보를 추적할 수 있는 관리형 테이블을 만들 수 있습니다.

`saveAsTable` 메소드는 테이블을 읽고 데이터를 Spark 포맷으로 변환한 후 새로운 경로에 저장합니다. 새로운 실행 계획에 이러한 동작이 반영되어 있음을 알 수 있으며, Hive 의 기본 웨어하우스 경로에 데이터를 저장하는 것을 확인할 수 있습니다. 데이터 저장 경로를 변경하려면 SparkSession 을 생성할 때 *spark.sql.warehouse.dir* 속성에 원하는 디렉토리 경로를 설정합니다. 기본값은 */user/hive/warehouse* 입니다.

저장 경로 하위에서 데이터베이스 목록을 확인할 수 있습니다. `show tables IN databaseName` 쿼리를 사용해 특정 데이터베이스의 테이블을 확인할 수도 있습니다.

### Creating Tables

다양한 데이터소스를 사용해 테이블을 생성할 수 있습니다. Spark SQL 에서 전체 데이터 소스 API 를 재사용할 수 있는 독특한 기능을 지원합니다. 즉, 테이블을 정의한 다음 테이블에 데이터를 적재할 필요가 없습니다. Spark 는 실행 즉시 테이블을 생성합니다. 파일에 데이터를 읽을 때 모든 종류의 정교한 옵션을 지정할 수도 있습니다.

```sql
CREATE TABLE flights (
  DEST_COUNTRY_NAME STRING, ORIGIN_COUNTRY_NAME STRING, count LONG)
USING JSON OPTIONS (path '/data/flight-data/json/2015-summary.json')
)
```

여기서 `USING` 을 지정하지 않으면 기본값으로 Hive SerDe 설정을 사용합니다. Hive SerDe 는 Spark 의 자체 직렬화보다 훨씬 느리므로 테이블을 사용하는 Reader 와 Writer 성능에 영향을 미칩니다. Hive 사용자는 STORED AS 구문으로 Hive 테이블을 생성할 수 있습니다.

테이블의 특정 칼럼에 **코멘트(Comment)** 를 추가해 이해를 도울 수 있습니다.

```sql
CREATE TABLE flights_csv (
  DEST_COUNTRY_NAME STRING,
  ORIGIN_COUNTRY_NAME STRING COMMENT "remember, the US will be most prevalent",
  count LONG)
USING csv OPTIONS (header true, path '/data/flight-data/csv/2015-summary.csv')
```

또한, `SELECT` 쿼리의 결과로 테이블을 생성할 수 있습니다.

```sql
CREATE TABLE flights_from_select USING parquet AS SELECT * FROM flights
```

테이블이 없는 경우에만 생성하도록 지정할 수 있습니다. 마지막으로 파티셔닝된 데이터셋을 저장해 데이터 레이아웃을 제어할 수 있습니다.

```sql
CREATE TABLE partitioned_flights USING parquet PARTITIONED BY (DEST_COUNTRY_NAME)
AS SELECT DEST_COUNTRY_NAME, ORIGIN_COUNTRY_NAME, count FROM flights LIMIT 5
```

Spark 에 접속한 세션에서도 생성된 테이블을 사용할 수 있습니다. 임시 테이블 개념은 현재 Spark 에 존재하지 않습니다. 사용자는 임시 뷰를 만뜰어 이 기능을 사용할 수 있습니다.

### Creating External Tables

Hive 는 초기 빅데이터 SQL 시스템 중 하나였습니다. 그리고 Spark SQL 은 HiveQL 과 호환이 됩니다. 기존 Hive 쿼리문을 Spark SQL 에서 바로 사용할 수 있습니다.

다음은 **외부 테이블** 을 생성하는 예제입니다. Spark 는 외부 테이블의 메타 데이터를 관리합니다. 하지만 데이터 파일은 Spark 에서 관리하지 않습니다. `CREATE EXTERNAL TABLE` 구문을 사용해 외부 테이블을 생성할 수 있습니다.

다음 구문을 사용해 예제 데이터에 미리 저장된 데이터 파일의 내용을 테이블 형태로 볼 수 있습니다.

```sql
CREATE EXTERNAL TABLE hive_flights (
  DEST_COUNTRY_NAME STRING, ORIGIN_COUNTRY_NAME STRING, count LONG)
ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' LOCATION '/data/flight-data-hive/'
```

또한 `SELECT` 쿼리 결과를 이용해 외부 테이블을 생성할 수 있습니다.

```sql
CREATE EXTERNAL TABLE hive_flights_2
ROW FORMAT DELIMITED FIELDS TERMINATED BY ','
LOCATION '/data/flight-data-hive/' AS SELECT * FROM flights
```

### Inserting into Tables

데이터 삽입은 표준 SQL 을 따릅니다.

```sql
INSERT INTO flights_from_select
  SELECT DEST_COUNTRY_NAME, ORIGIN_COUNTRY_NAME, count FROM flights LIMIT 20
```

특정 파티션에만 저장하고 싶은 경우 파티션 명세를 추가할 수 있습니다. 쓰기 연산은 파티셔닝 스키마에 맞게 데이터를 저장합닏다. 하지만 마지막 파티션에만 데이터 파일이 추가됩니다.

```sql
INSERT INTO partitioned_flights
  PARTITION (DEST_COUNTRY_NAME="UNITED STATES")
  SELECT count, ORIGIN_COUNTRY_NAME FROM flights
  WHERE DEST_COUNTRY_NAME='UNITED STATES' LIMIT 12
```

### Describing Table Metadata

테이블 생성 시 코멘트를 추가할 수 있습니다. 추가된 코멘트를 확인하려면 `DESCRIBE` 구문을 사용합니다. `DESCRIBE` 구문은 테이블의 메타데이터 정보를 반환합니다.

```sql
DESCRIBE TABLE flights_csv
```

다음 명령을 사용해 파티셔닝 스키마 정보도 확인할 수 있습니다. 이 명령은 파티션된 테이블에서만 동작합니다.

```sql
SHOW PARTITIONS partitioned_flights
```

### Refreshing Table Metadata

테이블 메타데이터를 유지하는 것은 가장 최신의 데이터셋을 읽고 있다는 것을 보장할 수 있는 중요한 작업입니다.

테이블 메타데이터를 갱신할 수 있는 두 가지 명령이 있습니다. `REFRESH TABLE` 구문은 테이블과 관련된 모든 캐싱된 항목을 갱신합니다. 테이블이 이미 캐싱되어 있다면 다음번 스캔이 동작하는 시점에 다시 캐싱합니다.

```sql
REFRESH table partitioned_flights
```

다른 명령은 카탈로그에서 관리하는 테이블의 파티션 정보를 새로 고치는 `REPAIR TABLE` 입니다. 이 명령은 새로운 파티션 정보를 수집하는 데 초점을 맞춥니다. 예를 들어 수동으로 신규 파티션을 만든다면 테이블을 수리해야 합니다.

```sql
MSCK REPAIR TABLE partitioned_flights
```

### Dropping Tables

테이블은 삭제할 수 없습니다. 오로지 제거만 할 수 있습니다. `DROP` 키워드를 사용해 테이블을 제거합니다. 관리형 테이블을 제거하면 데이터와 테이블 정의 모두 제거됩니다.

```sql
DROP TABLE flights_csv;
```

존재하지 않는 테이블을 제거하려하면 오류가 ㅂ라생하빈다. 테이블이 존재하는 경우에만 제거하려면 `DROP TABLE IF EXISTS` 구문을 사용합니다.

```sql
DROP TABLE IF EXISTS flights_csv;
```

#### DROPPING UNMANAGED TABLES

외부테이블을 제거하면 데이터는 삭제되지 않지만, 외부 테이블명을 이용해 데이터를 조회할 수 없습니다.

### Caching Tables

DataFrame 처럼 테이블을 캐시하거나 캐시에서 제거할 수 있습니다.

```sql
CACHE TABLE flights
```

캐시에서 제거하는 방법은 다음과 같습니다.

```sql
UNCACHE TABLE FLIGHTS
```

## Views

뷰는 기존 테이블에 여러 트랜스포메이션 작업을 지정합니다. 기본적으로 뷰는 단순 쿼리 실행 계획일 뿐입니다. 뷰를 사용하면 쿼리 로직을 체계화하거나 재사용하기 편하게 만들 수 있습니다. Spark 는 뷰와 관련도딘 다양한 개념을 가지고 있습니다.

뷰는 데이터베이스에 설정하는 전역 뷰나 세션별 뷰가될 수 있습니다.

### Creating Views

최종 사용자에게 뷰는 테이블처럼 보입니다. 신규 경로에 모든 데이터를 다시 저장하는 대신 단순하게 쿼리 시점에 데이터소스에 트랜스포메이션을 수행합니다. `filter`, `select` 또는 대규모 `GROUP BY`, `ROLLUP` 같은 트랜스 포메이션이 이에 해당합니다. 다음은 목적지가 United States 인 항공운항 데이터를 보기 위한 뷰를 생성하는 예제입니다.

```sql
CREATE VIEW just_usa_view AS
  SELECT * FROM flights WHERE dest_country_name = 'United States'
```

테이블처럼 데이터베이스에 등록되지 않고 현재 세션에서만 사용할 수 있는 임시 뷰를 만들 수 있습니다.

```sql
CREATE TEMP VIEW just_usa_view_temp AS
  SELECT * FROM flights WHERE dest_country_name = 'United States'
```

**전역 임시 뷰(Global Temp View)** 도 만들 수 있습니다. 이는 데이터베이스에 상관없이 사용할 수 있으므로 전체 Spark 어플리케이션에서 볼 수 있습니다. 하지만 세션이 종료되면 뷰도 사라집니다.

```sql
CREATE GLOBAL TEMP VIEW just_usa_global_view_temp AS
  SELECT * FROM flights WHERE dest_country_name = 'United States'

SHOW TABLES
```

다음에 나오는 키워드를 사용해 생성된 뷰를 덮어쓸 수 있습니다. 임시 뷰와 일반 뷰 모두 덮어쓸 수 있습니다.

```sql
CREATE OR REPLACE TEMP VIEW just_usa_view_temp AS
  SELECT * FROM flights WHERE dest_country_name = 'United States'
```

다른 테이블과 동일한 방식으로 뷰를 사용할 수 있습니다.

```sql
SELECT * FROM just_usa_view_temp
```

뷰는 실질적으로 트랜스포메이션이며 Spark 는 쿼리가 실행될 때만 뷰에 정의된 트랜스포메이션을 수행합니다. 즉, 테이블의 데이터를 실제로 조회하는 경우에만 필터를 적용합니다. 사실 뷰는 기존 DataFrame 에서 새로운 DataFrame 을 만드는것과 동일합니다.

Spark DataFrame 과 Spark SQL 로 생성된 쿼리 샐힝 계획을 비교해 확인할 수 있습니다. DataFrame 에서는 다음 코드를 사용합니다.

```scala
val flights = spark.read.format("json")
  .load("/data/flight-data/json/2015-summary.json")
val just_usa_df = flights.where("dest_country_name = 'United States'")
just_usa_df.selectExpr("*").explain
```

SQL 사용 시 다음과 같은 쿼리를 실행합니다.

```sql
EXPLAIN SELECT * FROM just_usa_view
```

다음 쿼리를 사용할 수 있습니다.

```sql
EXPLAIN SELECT * FROM flights WHERE dest_country_name = 'United States'
```

따라서 DataFrame 이나 SQL 중 편한 방법을 사용하면 됩니다.

### Dropping Views

테이블을 제거하는 것과 동일한 방식으로 뷰를 제거할 수 있습니다. 제거하려는 대상을 테이블이 아닌 뷰로 지정하기만 하면 됩니다. 뷰와 테이블 제거의 핵심 차이점은 뷰는 어떤 데이터도 제거되지 않으며 뷰 정의만 제거된다는 겁니다.

```sql
DROP VIEW IF EXISTS just_usa_view;
```

## Databases

데이터베이스는 여러 테이블을 조직화하기 위한 도구입니다. 데이터베이스를 정의하지 않으면 Spark 는 기본 데이터베이스를 사용합니다. Spark 에서 실행하는 모든 SQL 명령문은 사용 중인 데이터베이스 범위에서 실행됩니다. 즉, 데이터베이스를 변경하면 이전에 생성한 모든 사용자 테이블이 변경하기 전의 데이터베이스에 속해 있으므로 기존 테이블 데이터를 조회하려면 다르게 쿼리해야합니다.

다음 명령을 사용해 전체 데이터베이스 목록을 조회할 수 있습니다.

```sql
SHOW DATABASES;
```

### Creating Databases

`CREATE DATABASE` 키워드를 사용하면 됩니다.

```sql
CREATE DATABASE some_db
```

### Setting the Database

`USE` 키워드 다음에 데이터베이스명을 붙여서 쿼리 수행에 필요한 데이터베이스를 설정할 수 있습니다.

```sql
USER some_db
```

모든 쿼리는 테이블 이름을 찾을 때 앞서 지정한 데이터베이스를 참조합니다. 하지만 다른 데이터베이스를 지정했기 때문에 정상 동작하던 쿼리가 실패하거나 다른 결과를 반환할 수도 있습니다.

```sql
SHOW tables;

SELECT * FROM flights -- fails with table/view not found
```

그러므로 올바른 접두사를 사용해 다른 데이터베이스의 테이블에 쿼리를 수행할 수 있습니다.

```sql
SELECT * FROM default.flights
```

아래 명령을 사용해 현재 어떤 데이터베이스를 사용 중인지 확인할 수 있습니다.

```sql
SELECT current_database()
```

다음 명령을 사용해 기본 데이터베이스로 돌아갈 수 있습니다.

```sql
USE default
```

### Dropping Databases

데이터베이스를 삭제하거나 제거하는것도 쉽습니다. `DROP DATABASE` 를 사용하면 됩니다.

```sql
DROP DATABASE IF EXISTS some_db;
```

## Select Statements

Spark 쿼리는 ANSI-SQL 요건을 만족합니다. 또한 `SELECT` 표현식의 구조를 확인할 수 있습니다.

```sql
SELECT [ALL|DISTINCT] named_expression[, named_expression, ...]
    FROM relation[, relation, ...]
    [lateral_view[, lateral_view, ...]]
    [WHERE boolean_expression]
    [aggregation [HAVING boolean_expression]]
    [ORDER BY sort_expressions]
    [CLUSTER BY expressions]
    [DISTRIBUTE BY expressions]
    [SORT BY sort_expressions]
    [WINDOW named_window[, WINDOW named_window, ...]]
    [LIMIT num_rows]

named_expression:
    : expression [AS alias]

relation:
    | join_relation
    | (table_name|query|relation) [sample] [AS alias]
    : VALUES (expressions)[, (expressions), ...]
          [AS (column_name[, column_name, ...])]

expressions:
    : expression[, expression, ...]

sort_expressions:
    : expression [ASC|DESC][, expression [ASC|DESC], ...]
```

### case…when…then Statements

SQL 쿼리의 값을 조건에 맞게 변경해야 할 수도 있습니다. `case...when...then...end` 구문을 사용해 조건에 맞게 처리할 수 있습니다. 이는 프로그래밍의 `if` 문과 동일한 처리를합니다.

```sql
SELECT
  CASE WHEN DEST_COUNTRY_NAME = 'UNITED STATES' THEN 1
       WHEN DEST_COUNTRY_NAME = 'Egypt' THEN 0
       ELSE -1 END
FROM partitioned_flights
```

## Advanced Topics

지금까지 데이터가 어디에 존재하는지, 어떻게 구성해야 하는지 알아봤습니다. 이제 데이터를 쿼리하는 방법을 알아보겠습니다. SQL 쿼리는 특정 명령 집합을 실행하도록 요청하는 SQL 구문입니다.

SQL 구문은 조작, 정의, 제어와 관련된 명령을 정의할 수 있습니다.

### Complex Types

복합 데이터 타입은 표준 SQL 에는 존재하지 않는 강력한 기능입니다. Spark SQL 에는 구조체, 리스트, 맵 세 가지 핵심 복합 데이터 타입이 존재합니다.

#### STRUCTS

구조체는 맵에 더 가까우며 Spark 에서 중첩 데이터를 생성하거나 쿼리하는 방법을 제공합니다. 구조체를 만들기 위해서는 여러 칼럼이나 표현식을 괄호로 묶기만 하면 됩니다.

```sql
CREATE VIEW IF NOT EXISTS nested_data AS
  SELECT (DEST_COUNTRY_NAME, ORIGIN_COUNTRY_NAME) as country, count FROM flights
```

다음과 같은 방법을 사용해 구조체 데이터를 조회할 수 있습니다.

```sql
SELECT * FROM nested_data
```

구조체의 개별 칼럼을 조회할 수 있습니다. **점(dot)** 을 사용하면 됩니다.

```sql
SELECT country.DEST_COUNTRY_NAME, count FROM nested_data
```

구조체의 이름과 모든 하위 칼럼을 지정해 모든 값을 조회할 수 있습니다. 하위 칼럼은 아니지만 모든 것을 마치 칼럼처럼 다룰 수 있기 때문에 다음과 같이 더 간단한 방법을 사용할 수도 있습니다.

```sql
SELECT country.*, count FROM nested_data
```

### Functions

Spark SQL 은 복합 데이터 타입 외에도 다양한 고급 함수를 제공합니다. DataFrame 함수 문서에서 모든 함수를 찾아볼 수 있습니다. 그러나 SQL 에서도 이러한 하뭇를 찾는 방법이 있습니다. Spark SQL 이 제공하는 전체 함수 목록을 확인하려면 `SHOW FUNCTIONS` 구문을 사용합니다.

```sql
SHOW FUNCTIONS
```

Spark 에 내장된 시스템 함수느 사용자 함수 중 어떤 유형의 함수 목록을 보고 싶은지 명확하게 지정할 수 있습니다.

```sql
SHOW SYSTEM FUNCTIONS
```

사용자 함수는 누군가 Spark 환경에 공개한 함수입니다. 이전에 설명한 사용자 정의 함수와 동일합니다

```sql
SHOW USER FUNCTIONS
```

모든 `SHOW` 명령에 `*` 문자가 포함된 문자열을 사용하여 결과를 필터링할 수 있습니다. 예를 들어 s 로 시작하는 모든 함수를 필터링 하는 방법은 아래와 같습니다.

```sql
SHOW FUNCTIONS "s*";
```

`LIKE` 키워드를 선택적으로 사용할 수 있습니다.

```sql
SHOW FUNCTIONS LIKE "collect*";
```

함수 목록을 확인하는 것은 매우 유용합니다. 개별 함수에 대해 더 자세히 알고 싶다면 `DESCRIBE` 키워드를 사용합니다. `DESCRIBE` 는 함수 설명과 사용법을 반환합니다.

#### USER-DEFINED FUNCTIONS

Spark 는 사용자 정의 함수를 정의하고 분산 환경에서 사용할 수 있는 기능을 제공합니다. 특정 언어를 사용해 함수를 개발한 다음 등록하여 함수를 정의할 수 있습니다.

```scala
def power3(number:Double):Double = number * number * number
spark.udf.register("power3", power3(_:Double):Double)
```

```sql
SELECT count, power3(count) FROM flights
```

Hive 의 `CREATE TEMPORARY FUNCTIONS` 구문을 사용해 함수를 등록할 수 있습니다.

### Subqueries

**서브쿼리(Subquery)** 를 사용하면 쿼리 안에 쿼리를 지정할 수 있습니다. 이 기능을 사용하면 SQL 에서 정교한 로직을 명시할 수 있습니다. Spark 에는 두 가지 기본 서브쿼리가 있습니다. **상호연관 서브쿼리(Correlated Subquery)** 는 서브쿼리의 정보를 보완하기 위해 쿼리의 외부 범위에 있는 일부 정보를 사용할 수 있습니다. **비상호연관 서브쿼리(Uncorrelated Subquery)** 는 외부 범위에 있는 정보를 사용하지 않습니다.

이러한 쿼리 모두 하나 이상의 결과를 반환할 수 있습니다. Spark 는 값에 따라 필터링 할 수 있는 **조건절 서브쿼리(Predicate Subquery)** 도 지원합니다.

#### UNCORRELATED PREDICATE SUBQUERIES

조건절 서브쿼리를 살펴보겠습니다. 두 개의 비상호연관 쿼리로 구성되어 있습니다. 첫 번째 비상호연관 쿼리는 데이터 중 상위 5 개의 목적지 국가 정보를 조회합니다.

```sql
SELECT dest_country_name FROM flights
GROUP BY dest_country_name ORDER BY sum(count) DESC LIMIT 5
```

쿼리 결과는 아래와 같습니다.

```
+-----------------+
|dest_country_name|
+-----------------+
|    United States|
|           Canada|
|           Mexico|
|   United Kingdom|
|            Japan|
+-----------------+
```

이제 서브쿼리를 필터 내부에 배치하고 이전 예제의 결과에 출발지 국가 정보가 존재하는지 확인할 수 있습니다.

```sql
SELECT * FROM flights
WHERE origin_country_name IN (SELECT dest_country_name FROM flights
      GROUP BY dest_country_name ORDER BY sum(count) DESC LIMIT 5)
```

이 쿼리는 외부 범위에 있는 어떤 관련 정보도 사용하고 있지 않으므로 비상호연관 관계입니다. 이 쿼리는 독자적으로 실행할 수 있습니다.

#### CORRELATED PREDICATE SUBQUERIES

상호연관 조건절 서브쿼리는 내부 쿼리에서 외부 범위에 있는 정보를 사용할 수 있습니다. 예를 들어 목적지 국가에서 되돌아 올 수 있는 항공편이 있는지 알고 싶다면 목적지 국가를 출발지 국가로, 출발지 국가를 목적지 국가로 하여 항공편이 있는지 확인합니다.

```sql
SELECT * FROM flights f1
WHERE EXISTS (SELECT 1 FROM flights f2
            WHERE f1.dest_country_name = f2.origin_country_name)
AND EXISTS (SELECT 1 FROM flights f2
            WHERE f2.dest_country_name = f1.origin_country_name)
```

`EXISTS` 키워드는 서브쿼리에 값이 존재하면 true 를 반환합니다. 앞에 `NOT` 연산자를 붙여 결과를 뒤집을 수 있습니다. 만약 `NOT` 연산자를 사용했다면 돌아올 수 없는 목적지로 가는 항공편 정보를 반환합니다.

#### UNCORRELATED SCALAR QUERIES

**비상호연관 스칼라 쿼리(Uncorrelated Scalar Query)** 를 사용하면 기존에 없던 일부 부가 정보를 가져올 수 있습니다. 예를 들어 데이터셋 `count` 칼럼의 최댓값을 결과에 포함하고 싶은 경우 다음과 같은 쿼리를 사용할 수 있습니다.

```sql
SELECT *, (SELECT max(count) FROM flights) AS maximum FROM flights
```

## Miscellaneous Features

Spark SQL 코드의 성능 최적화나 디버깅이 필요한 경우 이 내용이 관련될 수 있습니다.

### Configurations
### Setting Configuration Values in SQL

Spark SQL 설정은 다음과 같이 지정할 수 있습니다.

|Property Name|Default|Meaning|
|:--|:--|:--|
|`spark.sql.inMemoryColumnarStorage.compressed`|true|이 값을 true 로 설정하면 Spark SQL 은 데이터 통계를 기반으로 각 칼럼에 대해 압축 코덱을 자동으로 설정합니다.|
|`spark.sql.inMemoryColumnarStorage.batchSize`|10000|칼럼 기반의 캐싱에 대한 배치 크기를 조절합니다. 배치 크기가 클수록 메모리 사용량과 압축 성능이 향상되지만, 데이터 캐싱 시 `OutOfMemoryError` 가 발생할 수 있습니다.|
|`spark.sql.files.maxPartitionBytes`|128 MB|파일을 읽을 때 단일 파티션에 할당할 수 있는 최대 바이트 수를 정의한다.|
|`spark.sql.files.openCostInBytes`|4 MB|동시에 스캔할 수 있는 바이트 수, 파일을 여는 데 드는 예상 비용을 측정하는데 사용됩니다. 이 값은 다수의 파일을 파티션에 넣을 때 사용합니다. 작은 파일을 많이 가진 파티션이 더 큰 파일을 가진 파티션보다 더 좋은 성능을 낼 수 있도록 넉넉한 값을 설정하는게 좋습니다.|
|`spark.sql.broadcastTimeout`|300|브로드캐스트 조인 시 전체 워커 노드로 전파할 때 기다릴 최대 시간을 초 단위로 정의합니다.|
|`spark.sql.autoBroadcastJoinThreshold`|10 MB|조인 시 워커 노드로 전파될 테이블의 최대 크기를 바이트 단위로 설정합니다. 이 값을 -1 로 설정하면 브로드캐스트 조인을 비활성화합니다. 현재 `ANALYZE TABLE COMPUTE STATISTICS noscan` 명령이 실행된 Hive 메타스토어에 정의된 테이블만 통계를 사용할 수 있습니다.|
|`spark.sql.shuffle.partitions`|200|조인이나 집계 수행에 필요한 데이터를 셔플링할 때 사용할 파티션 수를 설정합니다.|

```sql
SET spark.sql.shuffle.partitions=20
```